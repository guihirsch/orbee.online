#!/usr/bin/env python3
"""
HLS Complete Analysis - An√°lise Completa de Mata Ciliar com Dados HLS
Script principal que integra todas as funcionalidades do notebook HLS.ipynb
"""

import os
import sys
import warnings
import hashlib
from datetime import datetime

# Configura√ß√µes de warnings
warnings.filterwarnings('ignore')

# Importar m√≥dulos locais
try:
    # Tentar imports relativos (quando usado como m√≥dulo)
    from .config_hls import get_config
    from .hls_analysis import (
        check_hls_coverage, load_aoi_data, search_hls_data, 
        select_best_item, convert_numpy_types
    )
    from .hls_ndvi_processing import load_and_process_hls_data, create_ndvi_composite
    from .hls_degradation_analysis import (
        analyze_riparian_forest_degradation, load_river_geometry_for_buffer,
        generate_points_from_real_ndvi
    )
    from .hls_export import (
        ensure_output_directory, export_geojson_results, export_geotiff_results,
        create_processing_log, validate_ndvi_consistency
    )
except ImportError:
    # Fallback para imports absolutos (quando executado diretamente)
    from config_hls import get_config
    from hls_analysis import (
        check_hls_coverage, load_aoi_data, search_hls_data, 
        select_best_item, convert_numpy_types
    )
    from hls_ndvi_processing import load_and_process_hls_data, create_ndvi_composite
    from hls_degradation_analysis import (
        analyze_riparian_forest_degradation, load_river_geometry_for_buffer,
        generate_points_from_real_ndvi
    )
    from hls_export import (
        ensure_output_directory, export_geojson_results, export_geotiff_results,
        create_processing_log, validate_ndvi_consistency
    )

# Carregar configura√ß√µes centralizadas
config = get_config()

# Configura√ß√µes globais (com fallback para compatibilidade)
REGION_NAME = "Sinimbu, Rio Grande do Sul, Brasil" 
BUFFER_DISTANCE = config['degradation']['buffer_distance']
START_DATE = config['search']['start_date']
END_DATE = config['search']['end_date']
CLOUD_COVERAGE_MAX = config['search']['cloud_coverage_max']
NDVI_CRITICAL_THRESHOLD = config['ndvi']['critical_threshold']
NDVI_MODERATE_THRESHOLD = config['ndvi']['moderate_threshold']
MIN_DISTANCE_POINTS = config['points']['min_distance']
MAX_POINTS_PER_SEVERITY = config['points']['max_per_severity']
BUFFER_DISTANCE_RIVER = config['degradation']['buffer_distance_river']

def generate_unique_point_id(lat, lon, ndvi_value=None, timestamp=None):
    """
    Gera um ID √∫nico para cada ponto baseado APENAS nas coordenadas geogr√°ficas
    Isso permite referenciar o mesmo local em an√°lises futuras
    
    Args:
        lat: Latitude do ponto (WGS84)
        lon: Longitude do ponto (WGS84)
        ndvi_value: Valor NDVI (n√£o usado no ID, apenas para compatibilidade)
        timestamp: Timestamp (n√£o usado no ID, apenas para compatibilidade)
    
    Returns:
        str: ID √∫nico no formato 'hls_point_<hash>' baseado apenas nas coordenadas
    """
    # Criar string √∫nica baseada APENAS nas coordenadas
    # Usar precis√£o de 6 casas decimais para coordenadas (aproximadamente 0.1m de precis√£o)
    # Isso garante que o mesmo local geogr√°fico sempre tenha o mesmo ID
    unique_string = f"{lat:.6f}_{lon:.6f}"
    
    # Gerar hash SHA-256 e usar primeiros 12 caracteres
    hash_object = hashlib.sha256(unique_string.encode())
    hash_hex = hash_object.hexdigest()[:12]
    
    return f"hls_point_{hash_hex}"

def ensure_unique_ids_for_points(critical_points_data):
    """
    Garante que todos os pontos cr√≠ticos tenham IDs √∫nicos baseados em coordenadas
    
    Args:
        critical_points_data: Dados dos pontos cr√≠ticos
    
    Returns:
        dict: Dados dos pontos com IDs √∫nicos garantidos
    """
    if not critical_points_data or 'critical_points' not in critical_points_data:
        return critical_points_data
    
    print("üîß Garantindo IDs √∫nicos baseados em coordenadas...")
    
    critical_points = critical_points_data['critical_points']
    updated_count = 0
    
    for point in critical_points:
        # Verificar se j√° tem ID v√°lido baseado em coordenadas
        current_id = point.get('id', '')
        
        if not current_id or not current_id.startswith('hls_point_'):
            # Gerar novo ID baseado nas coordenadas
            lat_wgs84 = point.get('lat_wgs84', point.get('lat', 0))
            lon_wgs84 = point.get('lon_wgs84', point.get('lon', 0))
            
            # Se as coordenadas est√£o em UTM, converter para WGS84
            if 'lat_wgs84' not in point and 'lon_wgs84' not in point:
                # Assumir que lat/lon est√£o em UTM e converter
                try:
                    from pyproj import Transformer
                    transformer = Transformer.from_crs("EPSG:32722", "EPSG:4326", always_xy=True)
                    lon_wgs84, lat_wgs84 = transformer.transform(point['lon'], point['lat'])
                except:
                    # Fallback: usar coordenadas como est√£o
                    lat_wgs84 = point.get('lat', 0)
                    lon_wgs84 = point.get('lon', 0)
            
            # Gerar ID √∫nico baseado nas coordenadas WGS84
            point_id = generate_unique_point_id(lat_wgs84, lon_wgs84)
            point['id'] = point_id
            updated_count += 1
            
            print(f"   üîß ID gerado para ponto: {point_id}")
    
    if updated_count > 0:
        print(f"‚úÖ {updated_count} pontos receberam IDs √∫nicos baseados em coordenadas")
    else:
        print("‚úÖ Todos os pontos j√° possuem IDs √∫nicos v√°lidos")
    
    return critical_points_data

def configure_region():
    """Configura a regi√£o para an√°lise"""
    print("üåç CONFIGURA√á√ÉO DA REGI√ÉO")
    print("=" * 40)
    print(f"üìç Regi√£o atual: {REGION_NAME}")
    print("\nüí° Para alterar a regi√£o, edite a vari√°vel REGION_NAME no in√≠cio do script")
    print("   Exemplos de regi√µes v√°lidas:")
    print("   - 'Sinimbu, Rio Grande do Sul, Brasil'")
    print("   - 'Caxias do Sul, Rio Grande do Sul, Brasil'")
    print("   - 'Porto Alegre, RS'")
    print("   - 'S√£o Paulo, SP'")
    print("   - 'Bras√≠lia, DF'")
    print("=" * 40)
    return REGION_NAME

def main():
    """Fun√ß√£o principal do script"""
    print("üöÄ Iniciando HLS Complete Analysis - An√°lise de Mata Ciliar")
    print("üì° Processamento de dados HLS (Harmonized Landsat Sentinel)")
    print("üåø Foco: Detec√ß√£o de degrada√ß√£o em mata ciliar")
    print("=" * 60)
    
    # Configurar regi√£o
    region = configure_region()

    # ETAPA 1: Carregamento da AOI
    print("\nüìç ETAPA 1: Carregamento da √Årea de Interesse")
    print("-" * 50)
    print(f"üåç Regi√£o configurada: {region}")
    
    try:
        # Carregar AOI com filtro preciso por regi√£o
        aoi_gdf, source_path = load_aoi_data(region_name=region)
        
        print(f"‚úÖ AOI carregada: {source_path}")
        print(f"üìä Informa√ß√µes da AOI:")
        print(f"   - Geometrias: {len(aoi_gdf)}")
        print(f"   - Tipo: {aoi_gdf.geometry.geom_type.iloc[0]}")
        print(f"   - CRS: {aoi_gdf.crs}")
        print(f"   - Bounds: {aoi_gdf.total_bounds}")
        
        # Verificar se a AOI tem metadados da regi√£o
        if 'region' in aoi_gdf.columns:
            print(f"   - Regi√£o: {aoi_gdf['region'].iloc[0]}")
        if 'total_rivers' in aoi_gdf.columns:
            print(f"   - Rios encontrados: {aoi_gdf['total_rivers'].iloc[0]}")
        if 'buffer_distance' in aoi_gdf.columns:
            print(f"   - Buffer aplicado: {aoi_gdf['buffer_distance'].iloc[0]}m")

        # Se a AOI j√° tem buffer aplicado, usar diretamente
        if 'buffer_distance' in aoi_gdf.columns and aoi_gdf['buffer_distance'].iloc[0] == BUFFER_DISTANCE:
            print(f"‚úÖ AOI j√° possui buffer de {BUFFER_DISTANCE}m aplicado")
            aoi_buffer_gdf = aoi_gdf
        else:
            # Criar buffer adicional se necess√°rio
            print(f"üåä Aplicando buffer adicional de mata ciliar ({BUFFER_DISTANCE}m)...")
            
            # Converter para UTM para buffer em metros
            centroid = aoi_gdf.geometry.centroid.iloc[0]
            utm_zone = int((centroid.x + 180) / 6) + 1
            utm_crs = f"EPSG:{32700 + utm_zone}" if centroid.y < 0 else f"EPSG:{32600 + utm_zone}"

            print(f"üó∫Ô∏è Convertendo para UTM: {utm_crs}")
            aoi_utm = aoi_gdf.to_crs(utm_crs)
            aoi_buffer_utm = aoi_utm.buffer(BUFFER_DISTANCE)
            aoi_buffer_gdf = aoi_utm.to_crs("EPSG:4326")

        # Calcular √°rea total
        area_km2 = aoi_buffer_gdf.geometry.area.sum() * 1e-10  # Convertendo de graus¬≤ para km¬≤
        print(f"üìè √Årea total da AOI: {area_km2:.2f} km¬≤")

        # Definir bounds para busca HLS
        bounds = aoi_buffer_gdf.total_bounds
        print(f"üéØ Bounds para busca HLS: {bounds}")

        # Verificar cobertura HLS
        hls_coverage_ok = check_hls_coverage(bounds)
        if not hls_coverage_ok:
            print("‚ö†Ô∏è Regi√£o pode ter cobertura HLS limitada")

    except Exception as e:
        print(f"‚ùå Erro ao carregar AOI: {e}")
        return

    # ETAPA 2: Busca de dados HLS
    print("\nüì° ETAPA 2: Busca de Dados HLS")
    print("-" * 50)
    
    try:
        print("üöÄ Iniciando busca HLS...")
        hls_items = search_hls_data(bounds, START_DATE, END_DATE, CLOUD_COVERAGE_MAX)

        if hls_items and len(hls_items) > 0:
            selected_hls_items = select_best_item(hls_items, max_items=3)
            print(f"\n‚úÖ {len(selected_hls_items)} itens HLS selecionados para processamento")
        else:
            print("\n‚ùå FALHA TOTAL: Nenhum item HLS encontrado")
            selected_hls_items = None

    except Exception as e:
        print(f"‚ùå Erro cr√≠tico na busca HLS: {e}")
        selected_hls_items = None

    # ETAPA 3: Processamento NDVI
    print("\nüåø ETAPA 3: Processamento NDVI")
    print("-" * 50)
    
    processed_hls_data = []
    
    if selected_hls_items:
        print("üöÄ Processando itens HLS selecionados...")

        for i, item in enumerate(selected_hls_items):
            print(f"\nüìä Processando item {i+1}/{len(selected_hls_items)}...")

            processed_data = load_and_process_hls_data(item, bounds)

            if processed_data:
                processed_hls_data.append(processed_data)
                print("   ‚úÖ Processamento conclu√≠do")
            else:
                print("   ‚ùå Processamento falhou")

        # Criar composi√ß√£o final
        if processed_hls_data:
            final_ndvi_data = create_ndvi_composite(processed_hls_data)
            print(f"\n‚úÖ Processamento NDVI conclu√≠do com {len(processed_hls_data)} itens")
        else:
            print("\n‚ùå Nenhum item HLS processado com sucesso")
            final_ndvi_data = None
    else:
        print("‚ùå Nenhum item HLS dispon√≠vel para processamento")
        final_ndvi_data = None

    # ETAPA 4: An√°lise de Degrada√ß√£o
    print("\nüåä ETAPA 4: An√°lise de Degrada√ß√£o da Mata Ciliar")
    print("-" * 50)
    
    if final_ndvi_data and 'aoi_buffer_gdf' in locals():
        print("üöÄ Iniciando an√°lise de degrada√ß√£o da mata ciliar...")

        degradation_analysis = analyze_riparian_forest_degradation(
            final_ndvi_data,
            aoi_buffer_gdf
        )

        if degradation_analysis:
            print("‚úÖ An√°lise de degrada√ß√£o conclu√≠da")
        else:
            print("‚ùå An√°lise de degrada√ß√£o falhou")
            degradation_analysis = None
    else:
        print("‚ùå Dados necess√°rios n√£o dispon√≠veis para an√°lise de degrada√ß√£o")
        degradation_analysis = None

    # ETAPA 5: Gera√ß√£o de Pontos Cr√≠ticos (apenas cr√≠ticos)
    print("\nüìç ETAPA 5: Gera√ß√£o de Pontos Cr√≠ticos (apenas cr√≠ticos)")
    print("-" * 50)
    
    if degradation_analysis:
        print("üöÄ Iniciando gera√ß√£o de pontos cr√≠ticos...")

        # Carregar geometria do rio
        river_gdf, river_buffer_geom = load_river_geometry_for_buffer()
        
        if river_buffer_geom is None:
            print("‚ö†Ô∏è Usando buffer da an√°lise de degrada√ß√£o como fallback")
            river_buffer_geom = degradation_analysis['buffer_geometry'].geometry.iloc[0]

        critical_points_data = generate_points_from_real_ndvi(
            degradation_analysis,
            river_buffer_geom,
            max_points_per_category=MAX_POINTS_PER_SEVERITY
        )

        # Validar consist√™ncia entre an√°lise e pontos cr√≠ticos
        if critical_points_data:
            validate_ndvi_consistency(degradation_analysis, critical_points_data)
            
            # GARANTIR IDs √∫nicos baseados em coordenadas para todos os pontos
            critical_points_data = ensure_unique_ids_for_points(critical_points_data)

        if critical_points_data and critical_points_data['total_points'] > 0:
            print("‚úÖ Gera√ß√£o de pontos cr√≠ticos conclu√≠da com sucesso")
            print(f"   üî¥ Total de pontos cr√≠ticos: {critical_points_data['total_points']}")
            print("   üÜî IDs √∫nicos baseados em coordenadas garantidos")
        else:
            print("‚ùå Nenhum ponto cr√≠tico gerado")
            critical_points_data = None
    else:
        print("‚ùå An√°lise de degrada√ß√£o n√£o dispon√≠vel para gerar pontos")
        critical_points_data = None

    # ETAPA 6: Exporta√ß√£o de Resultados
    print("\nüíæ ETAPA 6: Exporta√ß√£o de Resultados")
    print("-" * 50)
    
    if critical_points_data and degradation_analysis:
        print("üöÄ Iniciando exporta√ß√£o de resultados...")

        # Garantir diret√≥rio de sa√≠da
        if ensure_output_directory("."):
            # Caminhos de sa√≠da (usando configura√ß√µes centralizadas)
            geojson_path = config['export']['geojson_filename']
            geotiff_path = config['export']['geotiff_filename']
            log_path = config['export']['log_filename']

            print(f"üìç Caminhos de sa√≠da:")
            print(f"   üìÑ GeoJSON: {geojson_path}")
            print(f"   üó∫Ô∏è GeoTIFF: {geotiff_path}")
            print(f"   üìù Log: {log_path}")

            # Exportar GeoJSON
            geojson_success = export_geojson_results(
                critical_points_data,
                degradation_analysis,
                geojson_path
            )

            # Exportar GeoTIFF
            geotiff_success = export_geotiff_results(
                final_ndvi_data,
                degradation_analysis,
                geotiff_path
            )

            # Criar log
            log_success = create_processing_log(
                critical_points_data,
                degradation_analysis,
                final_ndvi_data,
                log_path
            )

            # Resumo final
            print(f"\nüéâ PROCESSAMENTO HLS CONCLU√çDO!")
            print("=" * 60)
            print(f"‚úÖ GeoJSON: {'Sucesso' if geojson_success else 'Falhou'}")
            print(f"‚úÖ GeoTIFF: {'Sucesso' if geotiff_success else 'Falhou'}")
            print(f"‚úÖ Log: {'Sucesso' if log_success else 'Falhou'}")

            # Verificar se arquivos foram realmente criados
            print(f"\nüîç VERIFICA√á√ÉO DE ARQUIVOS:")
            files_created = []

            if os.path.exists(geojson_path):
                size_mb = os.path.getsize(geojson_path) / (1024*1024)
                print(f"   ‚úÖ {geojson_path} ({size_mb:.2f} MB)")
                files_created.append(geojson_path)
            else:
                print(f"   ‚ùå {geojson_path} N√ÉO ENCONTRADO")

            if os.path.exists(geotiff_path):
                size_mb = os.path.getsize(geotiff_path) / (1024*1024)
                print(f"   ‚úÖ {geotiff_path} ({size_mb:.2f} MB)")
                files_created.append(geotiff_path)
            else:
                print(f"   ‚ùå {geotiff_path} N√ÉO ENCONTRADO")

            if os.path.exists(log_path):
                size_kb = os.path.getsize(log_path) / 1024
                print(f"   ‚úÖ {log_path} ({size_kb:.1f} KB)")
                files_created.append(log_path)
            else:
                print(f"   ‚ùå {log_path} N√ÉO ENCONTRADO")

            if files_created:
                print(f"\nüìÅ Arquivos criados na pasta atual:")
                for file_path in files_created:
                    print(f"   üìÑ {file_path}")

                print(f"\nüåê Pr√≥ximos passos:")
                print(f"   1. Copiar arquivos para a pasta public/ do seu projeto")
                print(f"   2. Testar visualiza√ß√£o no AOIViewer.jsx")
                print(f"   3. Validar pontos cr√≠ticos no mapa web")
            else:
                print(f"\n‚ùå NENHUM ARQUIVO FOI CRIADO!")
                print(f"   Verifique os erros nas etapas anteriores")

        else:
            print("‚ùå Falha ao criar diret√≥rio de sa√≠da")

    else:
        print("‚ùå Dados insuficientes para exporta√ß√£o")
        print("   Verifique se todas as etapas anteriores foram executadas com sucesso")

    print("\nüéØ Script HLS Complete Analysis finalizado!")
    print("üìã INSTRU√á√ïES DE USO:")
    print("=" * 50)
    print("1. Execute: python hls_complete_analysis.py")
    print("2. Para alterar a regi√£o, edite a vari√°vel REGION_NAME no in√≠cio do script")
    print("3. Os resultados ser√£o salvos na pasta atual")
    print("\nüåç REGI√ïES SUPORTADAS:")
    print("- Qualquer munic√≠pio brasileiro (ex: 'S√£o Paulo, SP')")
    print("- Regi√µes com rios mapeados no OpenStreetMap")
    print("- O script filtra automaticamente apenas rios dentro dos limites administrativos")
    print("\nüìä RESULTADOS GERADOS:")
    print("- critical_points_mata_ciliar.geojson: Pontos cr√≠ticos de degrada√ß√£o (apenas cr√≠ticos)")
    print("  üÜî Cada ponto possui ID √∫nico baseado em coordenadas geogr√°ficas")
    print("  üìç Permite refer√™ncia temporal do mesmo local em an√°lises futuras")
    print("- ndvi_mata_ciliar_wgs84_normalized.geotiff: Mapa NDVI processado")
    print("- processamento_notebook.log: Log detalhado do processamento")
    print("\nüîß CONFIGURA√á√ïES AVAN√áADAS:")
    print("- BUFFER_DISTANCE: Dist√¢ncia do buffer de mata ciliar (metros)")
    print("- START_DATE/END_DATE: Per√≠odo para busca de dados HLS")
    print("- CLOUD_COVERAGE_MAX: M√°xima cobertura de nuvens aceita (%)")
    print("- NDVI_CRITICAL_THRESHOLD: Limiar cr√≠tico de NDVI")
    print("- NDVI_MODERATE_THRESHOLD: Limiar moderado de NDVI")
    print("\nüÜî SISTEMA DE IDs √öNICOS:")
    print("- IDs baseados APENAS em coordenadas geogr√°ficas (WGS84)")
    print("- Precis√£o de 6 casas decimais (~0.1m de precis√£o)")
    print("- Mesmo local geogr√°fico sempre ter√° o mesmo ID")
    print("- Ideal para acompanhamento temporal e compara√ß√£o de NDVI")

if __name__ == "__main__":
    main()
