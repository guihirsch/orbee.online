{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "APzsFYtvTJWp"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ==========================================\n",
        "# HLS.ipynb - An√°lise de Mata Ciliar com Dados HLS (Harmonized Landsat Sentinel)\n",
        "# ==========================================\n",
        "# Este notebook processa dados HLS da NASA para an√°lise de mata ciliar:\n",
        "# - Busca autom√°tica de dados HLS via STAC API\n",
        "# - Processamento NDVI com m√°scaras de qualidade\n",
        "# - Detec√ß√£o de pontos cr√≠ticos de degrada√ß√£o\n",
        "# - Exporta√ß√£o de resultados para integra√ß√£o web\n",
        "# ==========================================\n",
        "\n",
        "print(\"üöÄ Iniciando HLS.ipynb - An√°lise de Mata Ciliar\")\n",
        "print(\"üì° Processamento de dados HLS (Harmonized Landsat Sentinel)\")\n",
        "print(\"üåø Foco: Detec√ß√£o de degrada√ß√£o em mata ciliar\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Instala√ß√£o de depend√™ncias ---\n",
        "print(\"üì¶ Instalando depend√™ncias...\")\n",
        "!pip install pystac-client planetary_computer rasterio rioxarray geopandas shapely matplotlib numpy requests folium contextily stackstac xarray dask -q\n",
        "\n",
        "# --- 2. Importa√ß√µes principais ---\n",
        "import pystac_client\n",
        "import planetary_computer as pc\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "from rasterio.features import rasterize\n",
        "from rasterio.transform import from_bounds\n",
        "import rioxarray as rxr\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import shape, Point, box\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importa√ß√µes para processamento HLS\n",
        "import stackstac\n",
        "import xarray as xr\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas e importadas com sucesso!\")\n",
        "print(\"üîß Vers√µes principais:\")\n",
        "print(f\"   - rasterio: {rasterio.__version__}\")\n",
        "print(f\"   - geopandas: {gpd.__version__}\")\n",
        "print(f\"   - numpy: {np.__version__}\")\n",
        "\n",
        "# @title\n",
        "# ==========================================\n",
        "# ETAPA 1: Carregamento e Prepara√ß√£o da AOI (√Årea de Interesse)\n",
        "# ==========================================\n",
        "\n",
        "print(\"üìç ETAPA 1: Carregamento da √Årea de Interesse\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def check_hls_coverage(bounds):\n",
        "    \"\"\"Verifica se a regi√£o tem cobertura HLS te√≥rica\"\"\"\n",
        "    minx, miny, maxx, maxy = bounds\n",
        "\n",
        "    # HLS tem cobertura global, mas com algumas limita√ß√µes\n",
        "    # Verifica√ß√µes b√°sicas de cobertura\n",
        "    print(\"üåç Verificando cobertura HLS para a regi√£o...\")\n",
        "\n",
        "    # Verificar se est√° dentro dos limites globais razo√°veis\n",
        "    if miny < -60 or maxy > 80:\n",
        "        print(\"‚ö†Ô∏è Regi√£o pode ter cobertura limitada (latitudes extremas)\")\n",
        "        return False\n",
        "\n",
        "    # Verificar se a regi√£o n√£o √© muito pequena\n",
        "    area_deg = (maxx - minx) * (maxy - miny)\n",
        "    if area_deg < 0.001:  # Muito pequena\n",
        "        print(\"‚ö†Ô∏è Regi√£o muito pequena - expandindo ligeiramente...\")\n",
        "        return False\n",
        "\n",
        "    # Verificar se n√£o √© muito grande\n",
        "    if area_deg > 10:  # Muito grande\n",
        "        print(\"‚ö†Ô∏è Regi√£o muito grande - pode ser necess√°rio dividir\")\n",
        "        return False\n",
        "\n",
        "    print(\"‚úÖ Regi√£o dentro dos par√¢metros esperados para HLS\")\n",
        "    return True\n",
        "\n",
        "# Configura√ß√µes\n",
        "AOI_FILE = \"export.geojson\"  # Arquivo padr√£o do projeto\n",
        "BUFFER_DISTANCE = 200  # Buffer de mata ciliar em metros\n",
        "CRS_WGS84 = \"EPSG:4326\"\n",
        "\n",
        "def load_aoi_data():\n",
        "    \"\"\"Carrega dados da AOI com m√∫ltiplas op√ß√µes de fonte\"\"\"\n",
        "\n",
        "    # Op√ß√£o 1: Tentar carregar arquivo local do projeto\n",
        "    local_paths = [\n",
        "        \"../../export.geojson\",\n",
        "        \"../data/export.geojson\",\n",
        "        \"export.geojson\",\n",
        "        \"../scripts/data/export.geojson\"\n",
        "    ]\n",
        "\n",
        "    for path in local_paths:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"üìÇ Carregando AOI local: {path}\")\n",
        "            with open(path) as f:\n",
        "                data = json.load(f)\n",
        "            gdf = gpd.GeoDataFrame.from_features(data[\"features\"], crs=CRS_WGS84)\n",
        "            return gdf, path\n",
        "\n",
        "    # Op√ß√£o 2: Upload manual (Google Colab)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"üì§ Arquivo local n√£o encontrado. Fa√ßa upload do arquivo GeoJSON:\")\n",
        "        uploaded = files.upload()\n",
        "        geojson_path = list(uploaded.keys())[0]\n",
        "\n",
        "        with open(geojson_path) as f:\n",
        "            data = json.load(f)\n",
        "        gdf = gpd.GeoDataFrame.from_features(data[\"features\"], crs=CRS_WGS84)\n",
        "        return gdf, geojson_path\n",
        "\n",
        "    except ImportError:\n",
        "        # Op√ß√£o 3: AOI de exemplo (Sinimbu/RS)\n",
        "        print(\"‚ö†Ô∏è Usando AOI de exemplo - Sinimbu/RS\")\n",
        "        example_coords = [\n",
        "            [-52.5, -29.4], [-52.4, -29.4],\n",
        "            [-52.4, -29.5], [-52.5, -29.5], [-52.5, -29.4]\n",
        "        ]\n",
        "        from shapely.geometry import Polygon\n",
        "        example_geom = Polygon(example_coords)\n",
        "        gdf = gpd.GeoDataFrame([1], geometry=[example_geom], crs=CRS_WGS84)\n",
        "        return gdf, \"exemplo_sinimbu\"\n",
        "\n",
        "# Carregar AOI\n",
        "try:\n",
        "    aoi_gdf, source_path = load_aoi_data()\n",
        "\n",
        "    # Validar e preparar AOI\n",
        "    print(f\"‚úÖ AOI carregada: {source_path}\")\n",
        "    print(f\"üìä Informa√ß√µes da AOI:\")\n",
        "    print(f\"   - Geometrias: {len(aoi_gdf)}\")\n",
        "    print(f\"   - Tipo: {aoi_gdf.geometry.geom_type.iloc[0]}\")\n",
        "    print(f\"   - CRS: {aoi_gdf.crs}\")\n",
        "    print(f\"   - Bounds: {aoi_gdf.total_bounds}\")\n",
        "\n",
        "    # Criar buffer para mata ciliar\n",
        "    print(f\"üåä Criando buffer de mata ciliar ({BUFFER_DISTANCE}m)...\")\n",
        "\n",
        "    # Converter para UTM para buffer em metros\n",
        "    # Estimar zona UTM baseada no centr√≥ide\n",
        "    centroid = aoi_gdf.geometry.centroid.iloc[0]\n",
        "    utm_zone = int((centroid.x + 180) / 6) + 1\n",
        "    utm_crs = f\"EPSG:{32700 + utm_zone}\" if centroid.y < 0 else f\"EPSG:{32600 + utm_zone}\"\n",
        "\n",
        "    print(f\"üó∫Ô∏è Convertendo para UTM: {utm_crs}\")\n",
        "    aoi_utm = aoi_gdf.to_crs(utm_crs)\n",
        "    aoi_buffer_utm = aoi_utm.buffer(BUFFER_DISTANCE)\n",
        "    aoi_buffer_gdf = gpd.GeoDataFrame(geometry=aoi_buffer_utm, crs=utm_crs).to_crs(CRS_WGS84)\n",
        "\n",
        "    # Calcular √°rea total\n",
        "    area_km2 = aoi_buffer_utm.area.sum() / 1_000_000\n",
        "    print(f\"üìè √Årea total com buffer: {area_km2:.2f} km¬≤\")\n",
        "\n",
        "    # Definir bounds para busca HLS\n",
        "    bounds = aoi_buffer_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
        "    print(f\"üéØ Bounds para busca HLS: {bounds}\")\n",
        "\n",
        "    # Verificar cobertura HLS\n",
        "    hls_coverage_ok = check_hls_coverage(bounds)\n",
        "    if not hls_coverage_ok:\n",
        "        print(\"‚ö†Ô∏è Regi√£o pode ter cobertura HLS limitada\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro ao carregar AOI: {e}\")\n",
        "    raise\n",
        "\n",
        "# @title\n",
        "# ==========================================\n",
        "# ETAPA 2: Busca e Download de Dados HLS via STAC API\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüì° ETAPA 2: Busca de Dados HLS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Configura√ß√µes de busca - CORRIGIDAS para Microsoft Planetary Computer\n",
        "START_DATE = \"2022-06-01\"  # Per√≠odo com dados HLS confirmados\n",
        "END_DATE = \"2022-09-30\"    # Fim do per√≠odo (ver√£o/outono)\n",
        "CLOUD_COVERAGE_MAX = 50    # Aumentado para 50% para encontrar mais dados\n",
        "\n",
        "# Nomes corretos das cole√ß√µes HLS no Microsoft Planetary Computer\n",
        "HLS_COLLECTIONS = [\n",
        "    \"hls2-l30\",  # HLS Landsat 30m v2.0 (encontrado no diagn√≥stico)\n",
        "    \"hls2-s30\"   # HLS Sentinel-2 30m v2.0 (encontrado no diagn√≥stico)\n",
        "]\n",
        "\n",
        "print(\"üîß CONFIGURA√á√ïES AJUSTADAS:\")\n",
        "print(f\"   üìÖ Per√≠odo ajustado para: {START_DATE} a {END_DATE}\")\n",
        "print(f\"   ‚òÅÔ∏è Toler√¢ncia de nuvens: {CLOUD_COVERAGE_MAX}%\")\n",
        "print(\"   üí° Motivo: Garantir disponibilidade de dados HLS\")\n",
        "print(\"\")\n",
        "print(\"üì° SOBRE OS DADOS HLS:\")\n",
        "print(\"   üõ∞Ô∏è Harmonized Landsat Sentinel-2 (HLS)\")\n",
        "print(\"   üìè Resolu√ß√£o: 30 metros\")\n",
        "print(\"   üîÑ Revisita: 2-3 dias (combinando Landsat + Sentinel-2)\")\n",
        "print(\"   üåç Cobertura: Global\")\n",
        "print(\"   üìÖ Dispon√≠vel desde: 2013 (Landsat) / 2015 (Sentinel-2)\")\n",
        "print(\"   üè¢ Hospedado por: Microsoft Planetary Computer\")\n",
        "print(\"   üîó Fonte: NASA LP DAAC\")\n",
        "\n",
        "def search_hls_data(bounds, start_date, end_date, max_cloud=50):\n",
        "    \"\"\"Busca dados HLS via Microsoft Planetary Computer STAC API com diagn√≥sticos avan√ßados\"\"\"\n",
        "\n",
        "    print(f\"üîç Buscando dados HLS...\")\n",
        "    print(f\"   üìÖ Per√≠odo: {start_date} a {end_date}\")\n",
        "    print(f\"   ‚òÅÔ∏è M√°x. nuvens: {max_cloud}%\")\n",
        "    print(f\"   üìç Bounds: {bounds}\")\n",
        "\n",
        "    try:\n",
        "        # Conectar ao cat√°logo STAC do Microsoft Planetary Computer\n",
        "        print(\"üåê Conectando ao Microsoft Planetary Computer...\")\n",
        "        print(\"   üì° Fonte: NASA HLS data via Microsoft Azure\")\n",
        "        print(\"   üîó Refer√™ncia: https://www.earthdata.nasa.gov/news/blog/harmonized-landsat-sentinel-2-hls-data-now-available-microsofts-planetary-computer\")\n",
        "\n",
        "        catalog = pystac_client.Client.open(\n",
        "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
        "            modifier=pc.sign_inplace\n",
        "        )\n",
        "        print(\"‚úÖ Conex√£o estabelecida com Microsoft Planetary Computer\")\n",
        "\n",
        "        # Verificar cole√ß√µes HLS dispon√≠veis no Microsoft Planetary Computer\n",
        "        print(\"üìã Verificando cole√ß√µes HLS no Microsoft Planetary Computer...\")\n",
        "        print(\"   üîç Buscando: HLS Landsat 30m e HLS Sentinel-2 30m\")\n",
        "\n",
        "        available_collections = []\n",
        "\n",
        "        # Primeiro, listar todas as cole√ß√µes para debug\n",
        "        try:\n",
        "            all_collections = list(catalog.get_collections())\n",
        "            hls_related = [c.id for c in all_collections if 'hls' in c.id.lower()]\n",
        "            if hls_related:\n",
        "                print(f\"   üîç Cole√ß√µes HLS encontradas: {hls_related}\")\n",
        "            else:\n",
        "                print(\"   ‚ö†Ô∏è Nenhuma cole√ß√£o HLS encontrada, listando todas dispon√≠veis...\")\n",
        "                all_ids = [c.id for c in all_collections[:10]]  # Primeiras 10\n",
        "                print(f\"   üìã Primeiras cole√ß√µes: {all_ids}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Erro ao listar cole√ß√µes: {e}\")\n",
        "\n",
        "        # Usar os nomes corretos encontrados no diagn√≥stico\n",
        "        for collection_id in HLS_COLLECTIONS:\n",
        "            try:\n",
        "                collection = catalog.get_collection(collection_id)\n",
        "                available_collections.append(collection_id)\n",
        "                print(f\"   ‚úÖ {collection_id}: Dispon√≠vel\")\n",
        "\n",
        "                # Mostrar informa√ß√µes detalhadas\n",
        "                if hasattr(collection, 'extent') and collection.extent.temporal:\n",
        "                    temporal_extent = collection.extent.temporal.intervals[0]\n",
        "                    print(f\"      üìÖ Per√≠odo: {temporal_extent[0]} a {temporal_extent[1]}\")\n",
        "\n",
        "                if hasattr(collection, 'description'):\n",
        "                    desc = collection.description[:100] + \"...\" if len(collection.description) > 100 else collection.description\n",
        "                    print(f\"      üìù Descri√ß√£o: {desc}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå {collection_id}: N√£o dispon√≠vel ({e})\")\n",
        "\n",
        "        if not available_collections:\n",
        "            print(\"‚ùå Nenhuma cole√ß√£o HLS dispon√≠vel!\")\n",
        "            return None\n",
        "\n",
        "        # Buscar itens HLS\n",
        "        all_items = []\n",
        "\n",
        "        for collection in available_collections:\n",
        "            print(f\"\\nüõ∞Ô∏è Buscando cole√ß√£o: {collection}\")\n",
        "\n",
        "            try:\n",
        "                # Busca inicial sem filtro de nuvens\n",
        "                print(\"   üîç Busca inicial (sem filtro de nuvens)...\")\n",
        "                search_initial = catalog.search(\n",
        "                    collections=[collection],\n",
        "                    bbox=bounds,\n",
        "                    datetime=f\"{start_date}/{end_date}\"\n",
        "                )\n",
        "\n",
        "                initial_items = list(search_initial.items())\n",
        "                print(f\"   üìä Total de itens no per√≠odo: {len(initial_items)}\")\n",
        "\n",
        "                if len(initial_items) == 0:\n",
        "                    print(\"   ‚ö†Ô∏è Nenhum item encontrado no per√≠odo especificado\")\n",
        "\n",
        "                    # Tentar per√≠odo mais amplo\n",
        "                    print(\"   üîÑ Tentando per√≠odo mais amplo (2021-2023)...\")\n",
        "                    search_extended = catalog.search(\n",
        "                        collections=[collection],\n",
        "                        bbox=bounds,\n",
        "                        datetime=\"2021-01-01/2023-12-31\"\n",
        "                    )\n",
        "\n",
        "                    extended_items = list(search_extended.items())\n",
        "                    print(f\"   üìä Itens no per√≠odo estendido: {len(extended_items)}\")\n",
        "\n",
        "                    if len(extended_items) > 0:\n",
        "                        print(\"   üí° Dados dispon√≠veis em outros per√≠odos:\")\n",
        "                        for item in extended_items[:3]:\n",
        "                            date = item.properties.get(\"datetime\", \"N/A\")[:10]\n",
        "                            cloud = item.properties.get(\"eo:cloud_cover\", \"N/A\")\n",
        "                            print(f\"      - {date} | ‚òÅÔ∏è {cloud}%\")\n",
        "                    continue\n",
        "\n",
        "                # Aplicar filtro de nuvens\n",
        "                print(f\"   ‚òÅÔ∏è Aplicando filtro de nuvens (< {max_cloud}%)...\")\n",
        "                filtered_items = []\n",
        "\n",
        "                for item in initial_items:\n",
        "                    cloud_cover = item.properties.get(\"eo:cloud_cover\", 100)\n",
        "                    if cloud_cover < max_cloud:\n",
        "                        filtered_items.append(item)\n",
        "\n",
        "                print(f\"   ‚úÖ Itens ap√≥s filtro: {len(filtered_items)}\")\n",
        "                all_items.extend(filtered_items)\n",
        "\n",
        "                # Mostrar alguns exemplos\n",
        "                if filtered_items:\n",
        "                    print(\"   üèÜ Melhores itens desta cole√ß√£o:\")\n",
        "                    sorted_items = sorted(filtered_items, key=lambda x: x.properties.get(\"eo:cloud_cover\", 100))\n",
        "                    for i, item in enumerate(sorted_items[:3]):\n",
        "                        cloud_cover = item.properties.get(\"eo:cloud_cover\", \"N/A\")\n",
        "                        date = item.properties.get(\"datetime\", \"N/A\")[:10]\n",
        "                        print(f\"      {i+1}. {date} | ‚òÅÔ∏è {cloud_cover}%\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Erro na busca {collection}: {e}\")\n",
        "                import traceback\n",
        "                print(f\"   üîç Detalhes: {traceback.format_exc()}\")\n",
        "                continue\n",
        "\n",
        "        if not all_items:\n",
        "            print(\"\\n‚ùå DIAGN√ìSTICO: Nenhum item HLS encontrado!\")\n",
        "            print(\"üîç Poss√≠veis causas:\")\n",
        "            print(\"   1. Regi√£o fora da cobertura HLS\")\n",
        "            print(\"   2. Per√≠odo sem dados dispon√≠veis\")\n",
        "            print(\"   3. Filtros muito restritivos\")\n",
        "            print(\"   4. Problemas de conectividade\")\n",
        "\n",
        "            print(\"\\nüí° Sugest√µes:\")\n",
        "            print(\"   - Tente um per√≠odo diferente (2021-2022)\")\n",
        "            print(\"   - Aumente a toler√¢ncia de nuvens\")\n",
        "            print(\"   - Verifique se a regi√£o tem cobertura HLS\")\n",
        "            return None\n",
        "\n",
        "        # Ordenar por cobertura de nuvens\n",
        "        all_items.sort(key=lambda x: x.properties.get(\"eo:cloud_cover\", 100))\n",
        "\n",
        "        print(f\"\\nüìä RESULTADO FINAL:\")\n",
        "        print(f\"   ‚úÖ Total de itens encontrados: {len(all_items)}\")\n",
        "        print(f\"   üèÜ Melhores cenas (menor cobertura de nuvens):\")\n",
        "\n",
        "        for i, item in enumerate(all_items[:5]):\n",
        "            cloud_cover = item.properties.get(\"eo:cloud_cover\", \"N/A\")\n",
        "            date = item.properties.get(\"datetime\", \"N/A\")[:10]\n",
        "            collection = item.collection_id\n",
        "            print(f\"      {i+1}. {collection} | {date} | ‚òÅÔ∏è {cloud_cover}%\")\n",
        "\n",
        "        return all_items\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro cr√≠tico na busca HLS: {e}\")\n",
        "        import traceback\n",
        "        print(f\"üîç Traceback completo:\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def select_best_item(items, max_items=3):\n",
        "    \"\"\"Seleciona os melhores itens HLS baseado em crit√©rios de qualidade\"\"\"\n",
        "\n",
        "    if not items:\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nüéØ Selecionando melhores itens (m√°x: {max_items})...\")\n",
        "\n",
        "    # Filtrar e ordenar\n",
        "    filtered_items = []\n",
        "\n",
        "    for item in items:\n",
        "        cloud_cover = item.properties.get(\"eo:cloud_cover\", 100)\n",
        "        date = item.properties.get(\"datetime\", \"\")\n",
        "\n",
        "        # Crit√©rios de sele√ß√£o\n",
        "        if cloud_cover <= CLOUD_COVERAGE_MAX:\n",
        "            filtered_items.append({\n",
        "                'item': item,\n",
        "                'cloud_cover': cloud_cover,\n",
        "                'date': date,\n",
        "                'score': 100 - cloud_cover  # Score simples baseado em nuvens\n",
        "            })\n",
        "\n",
        "    # Ordenar por score\n",
        "    filtered_items.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    # Selecionar os melhores\n",
        "    selected_items = filtered_items[:max_items]\n",
        "\n",
        "    print(\"‚úÖ Itens selecionados:\")\n",
        "    for i, sel in enumerate(selected_items):\n",
        "        item = sel['item']\n",
        "        print(f\"   {i+1}. {item.collection_id} | {sel['date'][:10]} | ‚òÅÔ∏è {sel['cloud_cover']}%\")\n",
        "\n",
        "    return [sel['item'] for sel in selected_items]\n",
        "\n",
        "def try_alternative_periods_and_regions(bounds, original_start, original_end):\n",
        "    \"\"\"Tenta per√≠odos e configura√ß√µes alternativas para encontrar dados HLS\"\"\"\n",
        "\n",
        "    print(\"\\nüîÑ TENTANDO ALTERNATIVAS...\")\n",
        "\n",
        "    # Per√≠odos alternativos conhecidos com dados HLS\n",
        "    alternative_periods = [\n",
        "        (\"2022-01-01\", \"2022-12-31\", \"Ano completo 2022\"),\n",
        "        (\"2021-06-01\", \"2021-09-30\", \"Ver√£o 2021\"),\n",
        "        (\"2023-01-01\", \"2023-06-30\", \"Primeiro semestre 2023\"),\n",
        "        (\"2020-06-01\", \"2020-09-30\", \"Ver√£o 2020\")\n",
        "    ]\n",
        "\n",
        "    for start_date, end_date, description in alternative_periods:\n",
        "        print(f\"\\nüîç Tentando per√≠odo: {description} ({start_date} a {end_date})\")\n",
        "\n",
        "        try:\n",
        "            items = search_hls_data(bounds, start_date, end_date, CLOUD_COVERAGE_MAX)\n",
        "            if items and len(items) > 0:\n",
        "                print(f\"‚úÖ SUCESSO! Encontrados {len(items)} itens no per√≠odo {description}\")\n",
        "                return items, start_date, end_date\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Falhou para {description}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Se ainda n√£o encontrou, tentar regi√£o ligeiramente expandida\n",
        "    print(\"\\nüó∫Ô∏è Tentando regi√£o expandida...\")\n",
        "    minx, miny, maxx, maxy = bounds\n",
        "    buffer = 0.1  # ~11km de buffer\n",
        "    expanded_bounds = [minx - buffer, miny - buffer, maxx + buffer, maxy + buffer]\n",
        "\n",
        "    try:\n",
        "        items = search_hls_data(expanded_bounds, \"2022-01-01\", \"2022-12-31\", 70)  # Mais tolerante\n",
        "        if items and len(items) > 0:\n",
        "            print(f\"‚úÖ SUCESSO com regi√£o expandida! Encontrados {len(items)} itens\")\n",
        "            return items, \"2022-01-01\", \"2022-12-31\"\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Regi√£o expandida tamb√©m falhou: {e}\")\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "# Executar busca com fallbacks inteligentes\n",
        "try:\n",
        "    print(\"üöÄ Iniciando busca HLS...\")\n",
        "    hls_items = search_hls_data(bounds, START_DATE, END_DATE, CLOUD_COVERAGE_MAX)\n",
        "    used_start_date = START_DATE\n",
        "    used_end_date = END_DATE\n",
        "\n",
        "    # Se n√£o encontrou dados, tentar alternativas\n",
        "    if not hls_items or len(hls_items) == 0:\n",
        "        print(\"\\nüîÑ Dados n√£o encontrados no per√≠odo original, tentando alternativas...\")\n",
        "        hls_items, used_start_date, used_end_date = try_alternative_periods_and_regions(\n",
        "            bounds, START_DATE, END_DATE\n",
        "        )\n",
        "\n",
        "    if hls_items and len(hls_items) > 0:\n",
        "        selected_hls_items = select_best_item(hls_items, max_items=3)\n",
        "        print(f\"\\n‚úÖ {len(selected_hls_items)} itens HLS selecionados para processamento\")\n",
        "        print(f\"üìÖ Per√≠odo utilizado: {used_start_date} a {used_end_date}\")\n",
        "\n",
        "        # Atualizar vari√°veis globais para uso posterior\n",
        "        START_DATE = used_start_date\n",
        "        END_DATE = used_end_date\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ùå FALHA TOTAL: Nenhum item HLS encontrado\")\n",
        "        print(\"üîç Poss√≠veis solu√ß√µes:\")\n",
        "        print(\"   1. Verificar se a regi√£o tem cobertura HLS\")\n",
        "        print(\"   2. Usar dados Sentinel-2 diretamente\")\n",
        "        print(\"   3. Tentar uma regi√£o diferente\")\n",
        "        print(\"   4. Verificar conectividade com Microsoft Planetary Computer\")\n",
        "        selected_hls_items = None\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro cr√≠tico na busca HLS: {e}\")\n",
        "    import traceback\n",
        "    print(\"üîç Traceback completo:\")\n",
        "    print(traceback.format_exc())\n",
        "    selected_hls_items = None\n",
        "\n",
        "# Fun√ß√£o para converter tipos NumPy para tipos Python nativos\n",
        "def convert_numpy_types(obj):\n",
        "    \"\"\"Converte tipos NumPy para tipos Python nativos recursivamente\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types(item) for item in obj]\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Executar an√°lise (assumindo que ndvi, final_transform, final_crs, gdf_buffer existem)\n",
        "print(\"\\nüîç Executando an√°lise com thresholds FIXOS...\")\n",
        "# critical_analysis_fixed = identify_critical_points_fixed_thresholds(ndvi, final_transform, final_crs, gdf_buffer)\n",
        "\n",
        "# Salvar GeoJSON com corre√ß√£o de tipos\n",
        "def save_corrected_geojson(analysis_data, filename):\n",
        "    \"\"\"Salva GeoJSON com tipos corrigidos\"\"\"\n",
        "    if not analysis_data:\n",
        "        print(\"‚ùå Nenhum dado para salvar\")\n",
        "        return\n",
        "\n",
        "    features = []\n",
        "\n",
        "    # Adicionar pontos cr√≠ticos\n",
        "    for point in analysis_data['critical']:\n",
        "        features.append({\n",
        "            \"type\": \"Feature\",\n",
        "            \"geometry\": {\n",
        "                \"type\": \"Point\",\n",
        "                \"coordinates\": [point['lon'], point['lat']]\n",
        "            },\n",
        "            \"properties\": {\n",
        "                \"severity\": point['severity'],\n",
        "                \"ndvi\": point['ndvi'],\n",
        "                \"description\": point['description'],\n",
        "                \"type\": \"critical_point\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Adicionar pontos moderados\n",
        "    for point in analysis_data['moderate']:\n",
        "        features.append({\n",
        "            \"type\": \"Feature\",\n",
        "            \"geometry\": {\n",
        "                \"type\": \"Point\",\n",
        "                \"coordinates\": [point['lon'], point['lat']]\n",
        "            },\n",
        "            \"properties\": {\n",
        "                \"severity\": point['severity'],\n",
        "                \"ndvi\": point['ndvi'],\n",
        "                \"description\": point['description'],\n",
        "                \"type\": \"moderate_point\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Criar GeoJSON\n",
        "    geojson_data = {\n",
        "        \"type\": \"FeatureCollection\",\n",
        "        \"features\": features,\n",
        "        \"metadata\": {\n",
        "            \"analysis_date\": pd.Timestamp.now().isoformat(),\n",
        "            \"buffer_distance\": \"500 meters each side\",\n",
        "            \"thresholds\": analysis_data['thresholds'],\n",
        "            \"total_critical_points\": len(analysis_data['critical']),\n",
        "            \"total_moderate_points\": len(analysis_data['moderate']),\n",
        "            \"method\": \"fixed_thresholds_corrected\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Converter tipos e salvar\n",
        "    geojson_clean = convert_numpy_types(geojson_data)\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(geojson_clean, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"‚úÖ GeoJSON salvo: {filename}\")\n",
        "    print(f\"üìä Total de pontos: {len(features)}\")\n",
        "\n",
        "print(\"\\nüéØ Notebook corrigido e pronto para uso!\")\n",
        "print(\"üìã Para usar:\")\n",
        "print(\"1. Execute as c√©lulas anteriores para carregar dados NDVI\")\n",
        "print(\"2. Execute esta c√©lula para an√°lise com thresholds fixos\")\n",
        "print(\"3. Use save_corrected_geojson(critical_analysis_fixed, 'output.geojson')\")\n",
        "\n",
        "# @title\n",
        "# ==========================================\n",
        "# ETAPA 3: Processamento NDVI com M√°scaras de Qualidade\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüåø ETAPA 3: Processamento NDVI\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Configura√ß√µes NDVI\n",
        "NDVI_CRITICAL_THRESHOLD = 0.2   # Limite para √°reas sem vegeta√ß√£o ou muito ralas\n",
        "NDVI_MODERATE_THRESHOLD = 0.5   # Limite para separar vegeta√ß√£o esparsa de densa\n",
        "MIN_VALID_PIXELS = 0.05         # M√≠nimo 5% de pixels v√°lidos\n",
        "\n",
        "def load_and_process_hls_data(item, aoi_bounds):\n",
        "    \"\"\"Carrega e processa dados HLS para c√°lculo NDVI\"\"\"\n",
        "\n",
        "    print(f\"üì• Processando item: {item.collection_id}\")\n",
        "    print(f\"   üìÖ Data: {item.properties.get('datetime', 'N/A')[:10]}\")\n",
        "    print(f\"   ‚òÅÔ∏è Nuvens: {item.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
        "\n",
        "    try:\n",
        "        # Definir bandas necess√°rias para NDVI - HLS v2.0\n",
        "        if \"hls2-l30\" in item.collection_id:  # HLS Landsat\n",
        "            red_band = \"B04\"    # Red (665nm)\n",
        "            nir_band = \"B05\"    # NIR (865nm) - Landsat usa B05 para NIR\n",
        "            qa_band = \"Fmask\"   # Quality mask\n",
        "            print(f\"   üõ∞Ô∏è Tipo: HLS Landsat (L30)\")\n",
        "        else:  # HLS Sentinel-2\n",
        "            red_band = \"B04\"    # Red (665nm)\n",
        "            nir_band = \"B08\"    # NIR (842nm) - Sentinel-2 usa B08 para NIR\n",
        "            qa_band = \"Fmask\"   # Quality mask\n",
        "            print(f\"   üõ∞Ô∏è Tipo: HLS Sentinel-2 (S30)\")\n",
        "\n",
        "        bands_to_load = [red_band, nir_band, qa_band]\n",
        "        print(f\"   üìä Bandas: {bands_to_load}\")\n",
        "\n",
        "        # Carregar dados HLS usando abordagem alternativa (rioxarray)\n",
        "        print(f\"   üì• Carregando dados HLS...\")\n",
        "\n",
        "        # Determinar CRS dos dados HLS baseado no tile\n",
        "        item_id = item.id\n",
        "        print(f\"   üó∫Ô∏è Item ID: {item_id}\")\n",
        "\n",
        "        # Extrair zona UTM do tile HLS\n",
        "        import re\n",
        "        tile_match = re.search(r'T(\\d{2})([A-Z])', item_id)\n",
        "\n",
        "        if tile_match:\n",
        "            zone_num = int(tile_match.group(1))\n",
        "            lat_band = tile_match.group(2)\n",
        "\n",
        "            if lat_band < 'N':\n",
        "                hls_crs = f\"EPSG:{32700 + zone_num}\"\n",
        "                hemisphere = \"Sul\"\n",
        "            else:\n",
        "                hls_crs = f\"EPSG:{32600 + zone_num}\"\n",
        "                hemisphere = \"Norte\"\n",
        "\n",
        "            print(f\"   üåç Zona UTM: {zone_num}{lat_band} ({hemisphere})\")\n",
        "        else:\n",
        "            hls_crs = \"EPSG:32722\"\n",
        "            print(f\"   ‚ö†Ô∏è Usando CRS padr√£o: 22S\")\n",
        "\n",
        "        print(f\"   üó∫Ô∏è CRS detectado: {hls_crs}\")\n",
        "\n",
        "        # Abordagem alternativa: carregar bandas individualmente\n",
        "        try:\n",
        "            print(\"   üîÑ Carregando bandas individualmente...\")\n",
        "\n",
        "            # Carregar cada banda separadamente\n",
        "            band_arrays = {}\n",
        "\n",
        "            for band in bands_to_load:\n",
        "                print(f\"      üìä Carregando banda {band}...\")\n",
        "\n",
        "                # Obter URL da banda\n",
        "                band_asset = item.assets[band]\n",
        "                band_url = band_asset.href\n",
        "\n",
        "                # Assinar URL se necess√°rio (Microsoft Planetary Computer)\n",
        "                if 'planetarycomputer' in band_url:\n",
        "                    band_url = pc.sign(band_url)\n",
        "\n",
        "                # Carregar com rioxarray\n",
        "                try:\n",
        "                    band_data = rxr.open_rasterio(\n",
        "                        band_url,\n",
        "                        chunks={'x': 512, 'y': 512}  # Chunking para performance\n",
        "                    )\n",
        "\n",
        "                    # Reprojetar para bounds se necess√°rio\n",
        "                    if str(band_data.rio.crs) != \"EPSG:4326\":\n",
        "                        # Reprojetar bounds para CRS da banda\n",
        "                        from pyproj import Transformer\n",
        "                        transformer = Transformer.from_crs(\"EPSG:4326\", band_data.rio.crs, always_xy=True)\n",
        "                        minx_proj, miny_proj = transformer.transform(aoi_bounds[0], aoi_bounds[1])\n",
        "                        maxx_proj, maxy_proj = transformer.transform(aoi_bounds[2], aoi_bounds[3])\n",
        "\n",
        "                        # Recortar para AOI\n",
        "                        band_data = band_data.rio.clip_box(minx_proj, miny_proj, maxx_proj, maxy_proj)\n",
        "                    else:\n",
        "                        # Recortar diretamente\n",
        "                        band_data = band_data.rio.clip_box(*aoi_bounds)\n",
        "\n",
        "                    band_arrays[band] = band_data.squeeze()\n",
        "                    print(f\"      ‚úÖ {band}: {band_data.shape}\")\n",
        "\n",
        "                except Exception as band_error:\n",
        "                    print(f\"      ‚ùå Erro ao carregar {band}: {band_error}\")\n",
        "                    raise band_error\n",
        "\n",
        "            # Verificar se todas as bandas foram carregadas\n",
        "            if len(band_arrays) != len(bands_to_load):\n",
        "                raise Exception(\"Nem todas as bandas foram carregadas\")\n",
        "\n",
        "            # Extrair bandas individuais\n",
        "            red = band_arrays[red_band]\n",
        "            nir = band_arrays[nir_band]\n",
        "            qa = band_arrays[qa_band]\n",
        "\n",
        "            print(f\"   ‚úÖ Todas as bandas carregadas com sucesso!\")\n",
        "            print(f\"   üìê Dimens√µes Red: {red.shape}\")\n",
        "            print(f\"   üìê Dimens√µes NIR: {nir.shape}\")\n",
        "            print(f\"   üìê Dimens√µes QA: {qa.shape}\")\n",
        "            print(f\"   üó∫Ô∏è CRS: {red.rio.crs}\")\n",
        "\n",
        "        except Exception as load_error:\n",
        "            print(f\"   ‚ùå Erro no carregamento alternativo: {load_error}\")\n",
        "            raise load_error\n",
        "\n",
        "        # Bandas j√° foram extra√≠das na se√ß√£o anterior\n",
        "        # red, nir, qa j√° est√£o dispon√≠veis\n",
        "\n",
        "        print(f\"   üìä Dimens√µes das bandas:\")\n",
        "        print(f\"      - Red: {red.shape}, dtype: {red.dtype}\")\n",
        "        print(f\"      - NIR: {nir.shape}, dtype: {nir.dtype}\")\n",
        "        print(f\"      - QA: {qa.shape}, dtype: {qa.dtype}\")\n",
        "\n",
        "        # Aplicar escala HLS se necess√°rio (dados HLS j√° v√™m em reflect√¢ncia 0-1)\n",
        "        # Verificar se os dados precisam de escalonamento\n",
        "        red_max = float(np.nanmax(red.values))\n",
        "        nir_max = float(np.nanmax(nir.values))\n",
        "\n",
        "        print(f\"   üìà Valores m√°ximos: Red={red_max:.3f}, NIR={nir_max:.3f}\")\n",
        "\n",
        "        # Se os valores est√£o acima de 1, aplicar escala (dados em 0-10000)\n",
        "        if red_max > 1.5 or nir_max > 1.5:\n",
        "            print(\"   üî¢ Aplicando escala HLS (dividindo por 10000)...\")\n",
        "            red = red / 10000.0\n",
        "            nir = nir / 10000.0\n",
        "\n",
        "            # Verificar valores ap√≥s escala\n",
        "            red_scaled_max = float(np.nanmax(red.values))\n",
        "            nir_scaled_max = float(np.nanmax(nir.values))\n",
        "            print(f\"   üìä Valores ap√≥s escala: Red={red_scaled_max:.3f}, NIR={nir_scaled_max:.3f}\")\n",
        "        else:\n",
        "            print(\"   ‚úÖ Dados j√° est√£o em escala de reflect√¢ncia (0-1)\")\n",
        "\n",
        "        # Verificar se h√° valores v√°lidos nas bandas\n",
        "        red_valid = np.sum(np.isfinite(red.values) & (red.values > 0))\n",
        "        nir_valid = np.sum(np.isfinite(nir.values) & (nir.values > 0))\n",
        "        print(f\"   üìä Pixels com valores v√°lidos: Red={red_valid}, NIR={nir_valid}\")\n",
        "\n",
        "        # Aplicar m√°scara de qualidade (Fmask HLS)\n",
        "        # Fmask HLS: 0=clear, 1=water, 2=cloud_shadow, 3=snow, 4=cloud, 255=fill\n",
        "        print(\"   üé≠ Aplicando m√°scara de qualidade...\")\n",
        "\n",
        "        # Primeiro, vamos investigar os valores da m√°scara QA\n",
        "        qa_unique = np.unique(qa.values)\n",
        "        qa_counts = {val: int(np.sum(qa.values == val)) for val in qa_unique}\n",
        "        print(f\"   üìä Valores √∫nicos na m√°scara QA: {qa_unique}\")\n",
        "        print(f\"   üìà Contagem por valor: {qa_counts}\")\n",
        "\n",
        "        # M√°scara mais permissiva inicialmente para diagn√≥stico\n",
        "        # Incluir: 0=clear, 1=water, 2=cloud_shadow (pode ter vegeta√ß√£o)\n",
        "        valid_mask = (qa == 0) | (qa == 1) | (qa == 2)\n",
        "\n",
        "        # Se ainda n√£o h√° pixels v√°lidos, ser ainda mais permissivo\n",
        "        valid_pixels_count = int(np.sum(valid_mask.values))\n",
        "        print(f\"   üîç Pixels v√°lidos com m√°scara padr√£o: {valid_pixels_count}\")\n",
        "\n",
        "        if valid_pixels_count == 0:\n",
        "            print(\"   ‚ö†Ô∏è Nenhum pixel v√°lido com m√°scara padr√£o, tentando m√°scara mais permissiva...\")\n",
        "            # Incluir tudo exceto fill values (255) e nuvens densas (4)\n",
        "            valid_mask = (qa != 255) & (qa != 4)\n",
        "            valid_pixels_count = int(np.sum(valid_mask.values))\n",
        "            print(f\"   üîç Pixels v√°lidos com m√°scara permissiva: {valid_pixels_count}\")\n",
        "\n",
        "        if valid_pixels_count == 0:\n",
        "            print(\"   ‚ö†Ô∏è Ainda sem pixels v√°lidos, usando todos os pixels para diagn√≥stico...\")\n",
        "            valid_mask = qa >= 0  # Todos os pixels\n",
        "            valid_pixels_count = int(np.sum(valid_mask.values))\n",
        "            print(f\"   üîç Total de pixels: {valid_pixels_count}\")\n",
        "\n",
        "        # Calcular NDVI\n",
        "        print(\"   üßÆ Calculando NDVI...\")\n",
        "        denominator = nir + red\n",
        "\n",
        "        # Criar m√°scara combinada de forma segura\n",
        "        print(\"   üé≠ Criando m√°scara combinada...\")\n",
        "\n",
        "        # Componentes da m√°scara\n",
        "        denom_ok = (denominator != 0)\n",
        "        red_ok = (red >= 0) & np.isfinite(red)\n",
        "        nir_ok = (nir >= 0) & np.isfinite(nir)\n",
        "\n",
        "        # Diagn√≥sticos das m√°scaras\n",
        "        print(f\"   üìä Diagn√≥stico das m√°scaras:\")\n",
        "        print(f\"      - Denominador OK: {int(np.sum(denom_ok.values))}\")\n",
        "        print(f\"      - Red OK: {int(np.sum(red_ok.values))}\")\n",
        "        print(f\"      - NIR OK: {int(np.sum(nir_ok.values))}\")\n",
        "        print(f\"      - QA v√°lido: {valid_pixels_count}\")\n",
        "\n",
        "        # M√°scara combinada\n",
        "        combined_mask = denom_ok & valid_mask & red_ok & nir_ok\n",
        "        combined_valid = int(np.sum(combined_mask.values))\n",
        "        print(f\"      - M√°scara combinada: {combined_valid}\")\n",
        "\n",
        "        # Calcular NDVI com m√°scara segura\n",
        "        ndvi = xr.where(\n",
        "            combined_mask,\n",
        "            (nir - red) / denominator,\n",
        "            np.nan\n",
        "        )\n",
        "\n",
        "        # Se ainda n√£o h√° pixels v√°lidos, tentar NDVI sem m√°scara QA\n",
        "        if combined_valid == 0:\n",
        "            print(\"   üîÑ Tentando NDVI sem m√°scara de qualidade...\")\n",
        "            simple_mask = denom_ok & red_ok & nir_ok\n",
        "            simple_valid = int(np.sum(simple_mask.values))\n",
        "            print(f\"   üìä Pixels v√°lidos sem QA: {simple_valid}\")\n",
        "\n",
        "            if simple_valid > 0:\n",
        "                ndvi = xr.where(\n",
        "                    simple_mask,\n",
        "                    (nir - red) / denominator,\n",
        "                    np.nan\n",
        "                )\n",
        "                combined_valid = simple_valid\n",
        "\n",
        "        # Estat√≠sticas\n",
        "        valid_pixels = np.sum(~np.isnan(ndvi.values))\n",
        "        total_pixels = ndvi.size\n",
        "        valid_fraction = valid_pixels / total_pixels\n",
        "\n",
        "        print(f\"   üìä Estat√≠sticas NDVI:\")\n",
        "        print(f\"      - Pixels v√°lidos: {valid_pixels:,} ({valid_fraction:.1%})\")\n",
        "        print(f\"      - NDVI min: {np.nanmin(ndvi.values):.3f}\")\n",
        "        print(f\"      - NDVI max: {np.nanmax(ndvi.values):.3f}\")\n",
        "        print(f\"      - NDVI m√©dio: {np.nanmean(ndvi.values):.3f}\")\n",
        "\n",
        "        # Crit√©rio mais flex√≠vel para √°reas pequenas\n",
        "        min_threshold = min(MIN_VALID_PIXELS, 0.01)  # Pelo menos 1% ou o m√≠nimo configurado\n",
        "\n",
        "        if valid_fraction < min_threshold:\n",
        "            print(f\"   ‚ö†Ô∏è Poucos pixels v√°lidos ({valid_fraction:.1%} < {min_threshold:.1%})\")\n",
        "\n",
        "            # Se h√° pelo menos alguns pixels, continuar mesmo assim\n",
        "            if valid_pixels > 10:  # Pelo menos 10 pixels\n",
        "                print(f\"   üí° Continuando com {valid_pixels} pixels para an√°lise...\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå Insuficiente para an√°lise (apenas {valid_pixels} pixels)\")\n",
        "                return None\n",
        "\n",
        "        return {\n",
        "            'ndvi': ndvi,\n",
        "            'red': red,\n",
        "            'nir': nir,\n",
        "            'qa': qa,\n",
        "            'valid_mask': valid_mask,\n",
        "            'valid_fraction': valid_fraction,\n",
        "            'item': item,\n",
        "            'stats': {\n",
        "                'min': float(np.nanmin(ndvi.values)),\n",
        "                'max': float(np.nanmax(ndvi.values)),\n",
        "                'mean': float(np.nanmean(ndvi.values)),\n",
        "                'std': float(np.nanstd(ndvi.values)),\n",
        "                'valid_pixels': int(valid_pixels),\n",
        "                'total_pixels': int(total_pixels)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Erro no processamento: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_ndvi_composite(processed_items):\n",
        "    \"\"\"Cria composi√ß√£o NDVI a partir de m√∫ltiplos itens\"\"\"\n",
        "\n",
        "    if not processed_items:\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nüé® Criando composi√ß√£o NDVI de {len(processed_items)} itens...\")\n",
        "\n",
        "    # Se apenas um item, retornar diretamente\n",
        "    if len(processed_items) == 1:\n",
        "        return processed_items[0]\n",
        "\n",
        "    # M√∫ltiplos itens: criar composi√ß√£o baseada na qualidade\n",
        "    ndvi_arrays = []\n",
        "    weights = []\n",
        "\n",
        "    for item_data in processed_items:\n",
        "        ndvi_arrays.append(item_data['ndvi'])\n",
        "        # Peso baseado na fra√ß√£o de pixels v√°lidos\n",
        "        weights.append(item_data['valid_fraction'])\n",
        "\n",
        "    # Composi√ß√£o ponderada\n",
        "    ndvi_stack = xr.concat(ndvi_arrays, dim='time')\n",
        "    weights_array = xr.DataArray(weights, dims=['time'])\n",
        "\n",
        "    # M√©dia ponderada ignorando NaN\n",
        "    composite_ndvi = ndvi_stack.weighted(weights_array).mean(dim='time', skipna=True)\n",
        "\n",
        "    print(\"‚úÖ Composi√ß√£o NDVI criada\")\n",
        "    print(f\"   üìä NDVI final min: {np.nanmin(composite_ndvi.values):.3f}\")\n",
        "    print(f\"   üìä NDVI final max: {np.nanmax(composite_ndvi.values):.3f}\")\n",
        "    print(f\"   üìä NDVI final m√©dio: {np.nanmean(composite_ndvi.values):.3f}\")\n",
        "\n",
        "    return {\n",
        "        'ndvi': composite_ndvi,\n",
        "        'source_items': processed_items,\n",
        "        'stats': {\n",
        "            'min': float(np.nanmin(composite_ndvi.values)),\n",
        "            'max': float(np.nanmax(composite_ndvi.values)),\n",
        "            'mean': float(np.nanmean(composite_ndvi.values)),\n",
        "            'std': float(np.nanstd(composite_ndvi.values))\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Processar itens HLS selecionados\n",
        "processed_hls_data = []\n",
        "\n",
        "if selected_hls_items:\n",
        "    print(\"üöÄ Processando itens HLS selecionados...\")\n",
        "\n",
        "    for i, item in enumerate(selected_hls_items):\n",
        "        print(f\"\\nüìä Processando item {i+1}/{len(selected_hls_items)}...\")\n",
        "\n",
        "        processed_data = load_and_process_hls_data(item, bounds)\n",
        "\n",
        "        if processed_data:\n",
        "            processed_hls_data.append(processed_data)\n",
        "            print(\"   ‚úÖ Processamento conclu√≠do\")\n",
        "        else:\n",
        "            print(\"   ‚ùå Processamento falhou\")\n",
        "\n",
        "    # Criar composi√ß√£o final\n",
        "    if processed_hls_data:\n",
        "        final_ndvi_data = create_ndvi_composite(processed_hls_data)\n",
        "        print(f\"\\n‚úÖ Processamento NDVI conclu√≠do com {len(processed_hls_data)} itens\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Nenhum item HLS processado com sucesso\")\n",
        "        final_ndvi_data = None\n",
        "else:\n",
        "    print(\"‚ùå Nenhum item HLS dispon√≠vel para processamento\")\n",
        "    final_ndvi_data = None\n",
        "\n",
        "# @title\n",
        "# ==========================================\n",
        "# ETAPA 4: An√°lise de Mata Ciliar e Classifica√ß√£o de Severidade\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüåä ETAPA 4: An√°lise de Mata Ciliar\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def classify_vegetation_degradation(ndvi_value):\n",
        "    \"\"\"Classifica a cobertura/condi√ß√£o da vegeta√ß√£o baseada no NDVI (escala atualizada)\"\"\"\n",
        "\n",
        "    if np.isnan(ndvi_value):\n",
        "        return {\n",
        "            'level': 'no_data',\n",
        "            'color': '#808080',\n",
        "            'label': 'Sem Dados',\n",
        "            'severity': 'unknown'\n",
        "        }\n",
        "    elif ndvi_value < 0.0:\n",
        "        return {\n",
        "            'level': 'non_vegetated',\n",
        "            'color': '#0066CC',\n",
        "            'label': 'Sem vegeta√ß√£o (√°gua/nuvem/neve/rocha)',\n",
        "            'severity': 'non_vegetated'\n",
        "        }\n",
        "    elif ndvi_value < 0.2:\n",
        "        return {\n",
        "            'level': 'very_sparse',\n",
        "            'color': '#DC143C',\n",
        "            'label': 'Vegeta√ß√£o muito rala / solo exposto',\n",
        "            'severity': 'very_sparse'\n",
        "        }\n",
        "    elif ndvi_value < 0.5:\n",
        "        return {\n",
        "            'level': 'sparse',\n",
        "            'color': '#FF8C00',\n",
        "            'label': 'Vegeta√ß√£o esparsa / em regenera√ß√£o',\n",
        "            'severity': 'sparse'\n",
        "        }\n",
        "    elif ndvi_value < 0.8:\n",
        "        return {\n",
        "            'level': 'dense',\n",
        "            'color': '#228B22',\n",
        "            'label': 'Vegeta√ß√£o densa e saud√°vel',\n",
        "            'severity': 'dense'\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'level': 'extremely_dense',\n",
        "            'color': '#006400',\n",
        "            'label': 'Cobertura extremamente densa (rara)',\n",
        "            'severity': 'extremely_dense'\n",
        "        }\n",
        "\n",
        "def analyze_riparian_forest_degradation(ndvi_data, aoi_buffer_gdf):\n",
        "    \"\"\"Analisa degrada√ß√£o da mata ciliar dentro do buffer\"\"\"\n",
        "\n",
        "    if not ndvi_data or 'ndvi' not in ndvi_data:\n",
        "        print(\"‚ùå Dados NDVI n√£o dispon√≠veis para an√°lise\")\n",
        "        return None\n",
        "\n",
        "    ndvi_array = ndvi_data['ndvi']\n",
        "    print(f\"üìä Analisando degrada√ß√£o da mata ciliar...\")\n",
        "    print(f\"   üìê Dimens√µes NDVI: {ndvi_array.shape}\")\n",
        "\n",
        "    # Converter buffer para o CRS do NDVI\n",
        "    buffer_crs = ndvi_array.rio.crs\n",
        "    if buffer_crs is None:\n",
        "        print(\"‚ùå NDVI array n√£o possui CRS definido! Tentando definir manualmente...\")\n",
        "\n",
        "        # Verificar se temos dados de origem com CRS\n",
        "        if 'source_items' in ndvi_data and len(ndvi_data['source_items']) > 0:\n",
        "            # Usar CRS do primeiro item processado\n",
        "            first_item_data = ndvi_data['source_items'][0]\n",
        "            if 'ndvi' in first_item_data and first_item_data['ndvi'].rio.crs is not None:\n",
        "                buffer_crs = first_item_data['ndvi'].rio.crs\n",
        "                ndvi_array = ndvi_array.rio.write_crs(buffer_crs, inplace=True)\n",
        "                print(f\"‚úÖ CRS herdado dos dados HLS originais: {buffer_crs}\")\n",
        "            else:\n",
        "                # Fallback: usar UTM baseado na localiza√ß√£o do AOI\n",
        "                centroid = aoi_buffer_gdf.geometry.centroid.iloc[0]\n",
        "                utm_zone = int((centroid.x + 180) / 6) + 1\n",
        "                # For√ßar hemisf√©rio sul para esta regi√£o espec√≠fica\n",
        "                if centroid.y < 0:  # Hemisf√©rio sul\n",
        "                    buffer_crs = f\"EPSG:{32700 + utm_zone}\"\n",
        "                    print(f\"‚úÖ CRS definido para hemisf√©rio sul: {buffer_crs}\")\n",
        "                else:  # Hemisf√©rio norte\n",
        "                    buffer_crs = f\"EPSG:{32600 + utm_zone}\"\n",
        "                    print(f\"‚úÖ CRS definido para hemisf√©rio norte: {buffer_crs}\")\n",
        "                ndvi_array = ndvi_array.rio.write_crs(buffer_crs, inplace=True)\n",
        "        else:\n",
        "            # Inferir CRS baseado nos bounds do NDVI\n",
        "            ndvi_bounds = ndvi_array.rio.bounds()\n",
        "            ndvi_y_center = (ndvi_bounds[1] + ndvi_bounds[3]) / 2\n",
        "\n",
        "            # Se Y √© negativo, estamos no hemisf√©rio sul\n",
        "            if ndvi_y_center < 0:\n",
        "                # Estimar zona UTM baseada no AOI\n",
        "                centroid = aoi_buffer_gdf.geometry.centroid.iloc[0]\n",
        "                utm_zone = int((centroid.x + 180) / 6) + 1\n",
        "                buffer_crs = f\"EPSG:{32700 + utm_zone}\"  # Hemisf√©rio sul\n",
        "                print(f\"‚úÖ CRS inferido dos bounds NDVI (hemisf√©rio sul): {buffer_crs}\")\n",
        "            else:\n",
        "                # Hemisf√©rio norte\n",
        "                centroid = aoi_buffer_gdf.geometry.centroid.iloc[0]\n",
        "                utm_zone = int((centroid.x + 180) / 6) + 1\n",
        "                buffer_crs = f\"EPSG:{32600 + utm_zone}\"  # Hemisf√©rio norte\n",
        "                print(f\"‚úÖ CRS inferido dos bounds NDVI (hemisf√©rio norte): {buffer_crs}\")\n",
        "\n",
        "            ndvi_array = ndvi_array.rio.write_crs(buffer_crs, inplace=True)\n",
        "\n",
        "    print(f\"   üîÑ Reprojetando buffer de {aoi_buffer_gdf.crs} para {buffer_crs}...\")\n",
        "    buffer_reproj = aoi_buffer_gdf.to_crs(buffer_crs)\n",
        "\n",
        "    # Verificar se a reproje√ß√£o est√° no hemisf√©rio correto\n",
        "    reproj_bounds = buffer_reproj.total_bounds\n",
        "    reproj_y_center = (reproj_bounds[1] + reproj_bounds[3]) / 2\n",
        "\n",
        "    # Se esperamos hemisf√©rio sul mas temos coordenadas positivas, h√° problema\n",
        "    crs_code = int(str(buffer_crs).split(':')[-1])\n",
        "    is_south_utm = 32700 <= crs_code <= 32799\n",
        "    if is_south_utm and reproj_y_center > 0:\n",
        "        print(f\"   ‚ö†Ô∏è Problema detectado: CRS Sul mas coordenadas Norte!\")\n",
        "        print(f\"   üîß Corrigindo reproje√ß√£o...\")\n",
        "\n",
        "        # Corrigir coordenadas Y usando offset baseado nos dados NDVI\n",
        "        ndvi_bounds = ndvi_array.rio.bounds()\n",
        "        if ndvi_bounds[1] < 0:  # NDVI est√° no hemisf√©rio sul\n",
        "            print(f\"   üîß Aplicando corre√ß√£o de hemisf√©rio baseada nos dados NDVI...\")\n",
        "\n",
        "            # Calcular offset necess√°rio para colocar Y no hemisf√©rio correto\n",
        "            # A diferen√ßa entre hemisf√©rios em UTM √© aproximadamente 10,000,000m\n",
        "            y_offset = -10000000  # For√ßar hemisf√©rio sul\n",
        "\n",
        "            # Fun√ß√£o para corrigir coordenadas\n",
        "            def fix_coordinates(geom):\n",
        "                def coord_transform(x, y, z=None):\n",
        "                    return (x, y + y_offset, z) if z is not None else (x, y + y_offset)\n",
        "\n",
        "                from shapely.ops import transform\n",
        "                return transform(coord_transform, geom)\n",
        "\n",
        "            # Aplicar corre√ß√£o\n",
        "            corrected_geoms = buffer_reproj.geometry.apply(fix_coordinates)\n",
        "            buffer_reproj = gpd.GeoDataFrame(geometry=corrected_geoms, crs=buffer_crs)\n",
        "            print(f\"   ‚úÖ Coordenadas corrigidas para hemisf√©rio sul\")\n",
        "\n",
        "    print(f\"   üó∫Ô∏è CRS NDVI: {buffer_crs}\")\n",
        "    print(f\"   üåä Buffer reprojetado para an√°lise\")\n",
        "    print(f\"   üìç Buffer original bounds: {aoi_buffer_gdf.total_bounds}\")\n",
        "    print(f\"   üìç Buffer reprojetado bounds: {buffer_reproj.total_bounds}\")\n",
        "\n",
        "    # Criar m√°scara do buffer\n",
        "    try:\n",
        "        # Diagn√≥stico de bounds antes do clipping\n",
        "        print(f\"   üîç Diagn√≥stico de bounds:\")\n",
        "        ndvi_bounds = ndvi_array.rio.bounds()\n",
        "        buffer_bounds = buffer_reproj.total_bounds\n",
        "        print(f\"      - NDVI bounds: {ndvi_bounds}\")\n",
        "        print(f\"      - Buffer bounds: {buffer_bounds}\")\n",
        "\n",
        "        # Verificar se h√° intersec√ß√£o\n",
        "        ndvi_minx, ndvi_miny, ndvi_maxx, ndvi_maxy = ndvi_bounds\n",
        "        buf_minx, buf_miny, buf_maxx, buf_maxy = buffer_bounds\n",
        "\n",
        "        intersects = not (buf_maxx < ndvi_minx or buf_minx > ndvi_maxx or\n",
        "                         buf_maxy < ndvi_miny or buf_miny > ndvi_maxy)\n",
        "\n",
        "        print(f\"      - Intersec√ß√£o detectada: {intersects}\")\n",
        "\n",
        "        if not intersects:\n",
        "            print(\"‚ùå Buffer n√£o intersecta com dados NDVI!\")\n",
        "            print(\"üí° Poss√≠veis solu√ß√µes:\")\n",
        "            print(\"   1. Verificar se AOI est√° na regi√£o correta\")\n",
        "            print(\"   2. Expandir buffer ou √°rea de busca HLS\")\n",
        "            print(\"   3. Verificar CRS dos dados\")\n",
        "            return None\n",
        "\n",
        "        # Usar rioxarray para clip\n",
        "        print(f\"   ‚úÇÔ∏è Executando clipping...\")\n",
        "        ndvi_clipped = ndvi_array.rio.clip(buffer_reproj.geometry, buffer_reproj.crs)\n",
        "        print(f\"   ‚úÖ NDVI recortado para buffer da mata ciliar\")\n",
        "        print(f\"   üìê Dimens√µes ap√≥s clipping: {ndvi_clipped.shape}\")\n",
        "\n",
        "        # Estat√≠sticas dentro do buffer\n",
        "        valid_ndvi = ndvi_clipped.values[~np.isnan(ndvi_clipped.values)]\n",
        "\n",
        "        print(f\"   üìä Pixels ap√≥s clipping:\")\n",
        "        print(f\"      - Total: {ndvi_clipped.size}\")\n",
        "        print(f\"      - V√°lidos: {len(valid_ndvi)}\")\n",
        "        print(f\"      - NaN: {ndvi_clipped.size - len(valid_ndvi)}\")\n",
        "\n",
        "        if len(valid_ndvi) == 0:\n",
        "            print(\"‚ùå Nenhum pixel NDVI v√°lido dentro do buffer\")\n",
        "            print(\"üîç Poss√≠veis causas:\")\n",
        "            print(\"   1. Buffer muito pequeno\")\n",
        "            print(\"   2. Dados NDVI com muitos NaN na regi√£o\")\n",
        "            print(\"   3. Problema no processamento HLS\")\n",
        "            return None\n",
        "\n",
        "        # Calcular estat√≠sticas de degrada√ß√£o\n",
        "        critical_pixels = np.sum(valid_ndvi < NDVI_CRITICAL_THRESHOLD)\n",
        "        moderate_pixels = np.sum((valid_ndvi >= NDVI_CRITICAL_THRESHOLD) &\n",
        "                                (valid_ndvi < NDVI_MODERATE_THRESHOLD))\n",
        "        healthy_pixels = np.sum(valid_ndvi >= NDVI_MODERATE_THRESHOLD)\n",
        "        total_valid_pixels = len(valid_ndvi)\n",
        "\n",
        "        # Fra√ß√µes\n",
        "        critical_fraction = critical_pixels / total_valid_pixels\n",
        "        moderate_fraction = moderate_pixels / total_valid_pixels\n",
        "        healthy_fraction = healthy_pixels / total_valid_pixels\n",
        "\n",
        "        # Classifica√ß√£o geral da mata ciliar\n",
        "        if critical_fraction > 0.3:\n",
        "            overall_status = 'severely_degraded'\n",
        "            status_color = '#DC143C'\n",
        "        elif critical_fraction > 0.1 or moderate_fraction > 0.4:\n",
        "            overall_status = 'moderately_degraded'\n",
        "            status_color = '#FF8C00'\n",
        "        elif moderate_fraction > 0.2:\n",
        "            overall_status = 'at_risk'\n",
        "            status_color = '#FFD700'\n",
        "        else:\n",
        "            overall_status = 'healthy'\n",
        "            status_color = '#228B22'\n",
        "\n",
        "        # Estat√≠sticas detalhadas\n",
        "        stats = {\n",
        "            'total_pixels': int(total_valid_pixels),\n",
        "            'critical_pixels': int(critical_pixels),\n",
        "            'moderate_pixels': int(moderate_pixels),\n",
        "            'healthy_pixels': int(healthy_pixels),\n",
        "            'critical_fraction': float(critical_fraction),\n",
        "            'moderate_fraction': float(moderate_fraction),\n",
        "            'healthy_fraction': float(healthy_fraction),\n",
        "            'ndvi_min': float(np.min(valid_ndvi)),\n",
        "            'ndvi_max': float(np.max(valid_ndvi)),\n",
        "            'ndvi_mean': float(np.mean(valid_ndvi)),\n",
        "            'ndvi_std': float(np.std(valid_ndvi)),\n",
        "            'overall_status': overall_status,\n",
        "            'status_color': status_color\n",
        "        }\n",
        "\n",
        "        print(f\"üìä An√°lise de Degrada√ß√£o da Mata Ciliar:\")\n",
        "        print(f\"   üî¥ Cr√≠tico: {critical_pixels:,} pixels ({critical_fraction:.1%})\")\n",
        "        print(f\"   üü° Moderado: {moderate_pixels:,} pixels ({moderate_fraction:.1%})\")\n",
        "        print(f\"   üü¢ Saud√°vel: {healthy_pixels:,} pixels ({healthy_fraction:.1%})\")\n",
        "        print(f\"   üìà NDVI m√©dio: {stats['ndvi_mean']:.3f}\")\n",
        "        print(f\"   üè• Status geral: {overall_status}\")\n",
        "\n",
        "        return {\n",
        "            'ndvi_clipped': ndvi_clipped,\n",
        "            'buffer_geometry': buffer_reproj,\n",
        "            'statistics': stats,\n",
        "            'classification_function': classify_vegetation_degradation\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro na an√°lise de degrada√ß√£o: {e}\")\n",
        "        return None\n",
        "\n",
        "# Executar an√°lise de degrada√ß√£o\n",
        "if final_ndvi_data and 'aoi_buffer_gdf' in locals():\n",
        "    print(\"üöÄ Iniciando an√°lise de degrada√ß√£o da mata ciliar...\")\n",
        "\n",
        "    degradation_analysis = analyze_riparian_forest_degradation(\n",
        "        final_ndvi_data,\n",
        "        aoi_buffer_gdf\n",
        "    )\n",
        "\n",
        "    if degradation_analysis:\n",
        "        print(\"‚úÖ An√°lise de degrada√ß√£o conclu√≠da\")\n",
        "\n",
        "        # Visualiza√ß√£o opcional\n",
        "        try:\n",
        "            print(\"\\nüìä Criando visualiza√ß√£o da an√°lise...\")\n",
        "\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "            # Plot 1: NDVI original\n",
        "            ndvi_plot = final_ndvi_data['ndvi'].plot(\n",
        "                ax=axes[0],\n",
        "                cmap='RdYlGn',\n",
        "                vmin=-0.5,\n",
        "                vmax=1.0,\n",
        "                add_colorbar=True\n",
        "            )\n",
        "            axes[0].set_title('NDVI - Mata Ciliar')\n",
        "            axes[0].set_xlabel('Longitude')\n",
        "            axes[0].set_ylabel('Latitude')\n",
        "\n",
        "            # Plot 2: Histograma NDVI\n",
        "            valid_ndvi = degradation_analysis['ndvi_clipped'].values\n",
        "            valid_ndvi = valid_ndvi[~np.isnan(valid_ndvi)]\n",
        "\n",
        "            axes[1].hist(valid_ndvi, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "            axes[1].axvline(NDVI_CRITICAL_THRESHOLD, color='red', linestyle='--',\n",
        "                           label=f'Limite 0.2 (muito rala ‚Üí esparsa)')\n",
        "            axes[1].axvline(NDVI_MODERATE_THRESHOLD, color='orange', linestyle='--',\n",
        "                           label=f'Limite 0.5 (esparsa ‚Üí densa)')\n",
        "            axes[1].set_xlabel('NDVI')\n",
        "            axes[1].set_ylabel('Frequ√™ncia')\n",
        "            axes[1].set_title('Distribui√ß√£o NDVI - Mata Ciliar')\n",
        "            axes[1].legend()\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(\"‚úÖ Visualiza√ß√£o criada\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erro na visualiza√ß√£o: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå An√°lise de degrada√ß√£o falhou\")\n",
        "        degradation_analysis = None\n",
        "else:\n",
        "    print(\"‚ùå Dados necess√°rios n√£o dispon√≠veis para an√°lise de degrada√ß√£o\")\n",
        "    degradation_analysis = None\n",
        "\n",
        "# @title\n",
        "# ==========================================\n",
        "# ETAPA 5: Gera√ß√£o de Pontos Cr√≠ticos Restritos ao Buffer do Rio (CORRIGIDA)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüìç ETAPA 5: Gera√ß√£o de Pontos Cr√≠ticos (VERS√ÉO CORRIGIDA)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Configura√ß√µes para pontos cr√≠ticos\n",
        "MIN_DISTANCE_POINTS = 100  # Dist√¢ncia m√≠nima entre pontos (metros)\n",
        "MAX_POINTS_PER_SEVERITY = 50  # Reduzido para melhor performance\n",
        "BUFFER_DISTANCE_RIVER = 200  # Buffer do rio em metros\n",
        "SAMPLING_STEP = 3  # Para compatibilidade com fun√ß√£o de exporta√ß√£o\n",
        "\n",
        "def load_river_geometry_for_buffer():\n",
        "    \"\"\"Carrega geometria do rio para criar buffer preciso\"\"\"\n",
        "\n",
        "    # Caminhos poss√≠veis para o arquivo do rio\n",
        "    rio_paths = [\n",
        "        \"../../public/rio.geojson\",\n",
        "        \"../public/rio.geojson\",\n",
        "        \"rio.geojson\",\n",
        "        \"export.geojson\",\n",
        "        \"../data/export.geojson\"\n",
        "    ]\n",
        "\n",
        "    river_gdf = None\n",
        "\n",
        "    for rio_path in rio_paths:\n",
        "        if os.path.exists(rio_path):\n",
        "            print(f\"üìÇ Carregando rio: {rio_path}\")\n",
        "            try:\n",
        "                with open(rio_path, 'r', encoding='utf-8') as f:\n",
        "                    rio_data = json.load(f)\n",
        "                river_gdf = gpd.GeoDataFrame.from_features(rio_data['features'], crs='EPSG:4326')\n",
        "                print(f\"   ‚úÖ {len(river_gdf)} features do rio carregadas\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Erro ao carregar {rio_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "    if river_gdf is None:\n",
        "        print(\"‚ö†Ô∏è Arquivo do rio n√£o encontrado, usando AOI buffer existente\")\n",
        "        return None, None\n",
        "\n",
        "    # Unir geometrias do rio\n",
        "    try:\n",
        "        river_union = river_gdf.geometry.union_all()  # M√©todo novo\n",
        "    except AttributeError:\n",
        "        river_union = river_gdf.geometry.unary_union  # M√©todo antigo (deprecated)\n",
        "\n",
        "    print(f\"   üåä Geometrias unificadas: {type(river_union)}\")\n",
        "\n",
        "    # Converter para UTM para buffer preciso\n",
        "    centroid = river_union.centroid if hasattr(river_union, 'centroid') else river_union.geoms[0].centroid\n",
        "    utm_zone = int((centroid.x + 180) / 6) + 1\n",
        "    utm_crs = f\"EPSG:{32700 + utm_zone}\" if centroid.y < 0 else f\"EPSG:{32600 + utm_zone}\"\n",
        "\n",
        "    print(f\"   üó∫Ô∏è UTM CRS: {utm_crs}\")\n",
        "\n",
        "    # Criar buffer do rio\n",
        "    river_gdf_unified = gpd.GeoDataFrame([1], geometry=[river_union], crs='EPSG:4326')\n",
        "    river_utm = river_gdf_unified.to_crs(utm_crs)\n",
        "    river_buffer_utm = river_utm.buffer(BUFFER_DISTANCE_RIVER)\n",
        "    river_buffer_wgs84 = gpd.GeoDataFrame(geometry=river_buffer_utm, crs=utm_crs).to_crs('EPSG:4326')\n",
        "\n",
        "    buffer_geom = river_buffer_wgs84.geometry.iloc[0]\n",
        "\n",
        "    print(f\"   üìè Buffer de {BUFFER_DISTANCE_RIVER}m criado\")\n",
        "    print(f\"   üìç Bounds do buffer: {river_buffer_wgs84.total_bounds}\")\n",
        "\n",
        "    return river_gdf_unified, buffer_geom\n",
        "\n",
        "def generate_critical_points_buffer_constrained(degradation_analysis, max_points_per_category=50):\n",
        "    \"\"\"Gera pontos cr√≠ticos APENAS dentro do buffer do rio - VERS√ÉO CORRIGIDA\"\"\"\n",
        "\n",
        "    if not degradation_analysis:\n",
        "        print(\"‚ùå An√°lise de degrada√ß√£o n√£o dispon√≠vel\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üéØ Gerando pontos cr√≠ticos restritos ao buffer do rio...\")\n",
        "\n",
        "    # Carregar geometria do rio e criar buffer\n",
        "    river_gdf, river_buffer_geom = load_river_geometry_for_buffer()\n",
        "\n",
        "    if river_buffer_geom is None:\n",
        "        print(\"‚ö†Ô∏è Usando buffer da an√°lise de degrada√ß√£o como fallback\")\n",
        "        river_buffer_geom = degradation_analysis['buffer_geometry'].geometry.iloc[0]\n",
        "\n",
        "    classify_func = degradation_analysis['classification_function']\n",
        "\n",
        "    def calculate_distance(lat1, lon1, lat2, lon2):\n",
        "        \"\"\"Calcula dist√¢ncia em metros usando f√≥rmula de Haversine\"\"\"\n",
        "        R = 6371000  # Raio da Terra em metros\n",
        "        dlat = np.radians(lat2 - lat1)\n",
        "        dlon = np.radians(lon2 - lon1)\n",
        "        a = (np.sin(dlat/2)**2 +\n",
        "             np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2)**2)\n",
        "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "        return R * c\n",
        "\n",
        "    def is_too_close(new_point, existing_points, min_distance):\n",
        "        \"\"\"Verifica se um ponto est√° muito pr√≥ximo dos existentes\"\"\"\n",
        "        for existing in existing_points:\n",
        "            if calculate_distance(\n",
        "                new_point['lat'], new_point['lon'],\n",
        "                existing['lat'], existing['lon']\n",
        "            ) < min_distance:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def generate_points_from_real_ndvi(degradation_analysis, river_buffer_geom, max_points_per_category=50):\n",
        "    \"\"\"Gera pontos cr√≠ticos baseados no NDVI real da an√°lise de degrada√ß√£o\"\"\"\n",
        "    \n",
        "    if not degradation_analysis or 'ndvi_clipped' not in degradation_analysis:\n",
        "        print(\"‚ùå Dados NDVI n√£o dispon√≠veis para gera√ß√£o de pontos\")\n",
        "        return None\n",
        "    \n",
        "    ndvi_clipped = degradation_analysis['ndvi_clipped']\n",
        "    valid_ndvi = ndvi_clipped.values[~np.isnan(ndvi_clipped.values)]\n",
        "    \n",
        "    if len(valid_ndvi) == 0:\n",
        "        print(\"‚ùå Nenhum pixel NDVI v√°lido para gerar pontos\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üìä Gerando pontos baseados em {len(valid_ndvi)} pixels NDVI reais\")\n",
        "    \n",
        "    # Classificar pixels reais por severidade\n",
        "    critical_mask = valid_ndvi < NDVI_CRITICAL_THRESHOLD\n",
        "    moderate_mask = (valid_ndvi >= NDVI_CRITICAL_THRESHOLD) & (valid_ndvi < NDVI_MODERATE_THRESHOLD)\n",
        "    healthy_mask = valid_ndvi >= NDVI_MODERATE_THRESHOLD\n",
        "    \n",
        "    critical_pixels = valid_ndvi[critical_mask]\n",
        "    moderate_pixels = valid_ndvi[moderate_mask]\n",
        "    healthy_pixels = valid_ndvi[healthy_mask]\n",
        "    \n",
        "    print(f\"   üî¥ Pixels cr√≠ticos reais: {len(critical_pixels)}\")\n",
        "    print(f\"   üü° Pixels moderados reais: {len(moderate_pixels)}\")\n",
        "    print(f\"   üü¢ Pixels saud√°veis reais: {len(healthy_pixels)}\")\n",
        "    \n",
        "    # Fun√ß√£o auxiliar para encontrar coordenadas de pixels\n",
        "    def find_pixel_coordinates(ndvi_array, pixel_indices, n_points):\n",
        "        \"\"\"Encontra coordenadas de pixels espec√≠ficos\"\"\"\n",
        "        if len(pixel_indices) == 0:\n",
        "            return []\n",
        "        \n",
        "        # Amostrar pixels se necess√°rio\n",
        "        if len(pixel_indices) > n_points:\n",
        "            sampled_indices = np.random.choice(pixel_indices, n_points, replace=False)\n",
        "        else:\n",
        "            sampled_indices = pixel_indices\n",
        "        \n",
        "        points = []\n",
        "        transform = ndvi_array.rio.transform()\n",
        "        \n",
        "        for idx in sampled_indices:\n",
        "            # Converter √≠ndice linear para 2D\n",
        "            y_idx, x_idx = np.unravel_index(idx, ndvi_array.shape)\n",
        "            \n",
        "            # Converter para coordenadas geogr√°ficas\n",
        "            lon, lat = rasterio.transform.xy(transform, y_idx, x_idx)\n",
        "            \n",
        "            # Verificar se est√° dentro do buffer do rio\n",
        "            point_geom = Point(lon, lat)\n",
        "            if river_buffer_geom.contains(point_geom):\n",
        "                points.append({\n",
        "                    'lat': lat,\n",
        "                    'lon': lon,\n",
        "                    'ndvi': float(ndvi_array.values.flat[idx]),\n",
        "                    'severity': 'critical' if idx in np.where(critical_mask)[0] else \n",
        "                               'moderate' if idx in np.where(moderate_mask)[0] else 'healthy',\n",
        "                    'level': 'very_sparse' if idx in np.where(critical_mask)[0] else\n",
        "                            'sparse' if idx in np.where(moderate_mask)[0] else 'dense',\n",
        "                    'color': '#DC143C' if idx in np.where(critical_mask)[0] else\n",
        "                            '#FF8C00' if idx in np.where(moderate_mask)[0] else '#228B22',\n",
        "                    'label': 'Vegeta√ß√£o muito rala / solo exposto' if idx in np.where(critical_mask)[0] else\n",
        "                            'Vegeta√ß√£o esparsa / em regenera√ß√£o' if idx in np.where(moderate_mask)[0] else\n",
        "                            'Vegeta√ß√£o densa e saud√°vel',\n",
        "                    'description': f\"√Årea real - NDVI {ndvi_array.values.flat[idx]:.3f}\",\n",
        "                    'source': 'real_ndvi_analysis'\n",
        "                })\n",
        "        \n",
        "        return points\n",
        "    \n",
        "    # Gerar pontos para cada categoria\n",
        "    critical_points = find_pixel_coordinates(ndvi_clipped, np.where(critical_mask)[0], \n",
        "                                           min(max_points_per_category, len(critical_pixels)))\n",
        "    moderate_points = find_pixel_coordinates(ndvi_clipped, np.where(moderate_mask)[0], \n",
        "                                           min(max_points_per_category, len(moderate_pixels)))\n",
        "    healthy_points = find_pixel_coordinates(ndvi_clipped, np.where(healthy_mask)[0], \n",
        "                                          min(max_points_per_category, len(healthy_pixels)))\n",
        "    \n",
        "    total_points = len(critical_points) + len(moderate_points) + len(healthy_points)\n",
        "    \n",
        "    print(f\"\\\\nüìä Pontos gerados com NDVI real:\")\n",
        "    print(f\"   üî¥ Cr√≠ticos: {len(critical_points)}\")\n",
        "    print(f\"   üü° Moderados: {len(moderate_points)}\")\n",
        "    print(f\"   üü¢ Saud√°veis: {len(healthy_points)}\")\n",
        "    print(f\"   üìä Total: {total_points}\")\n",
        "    \n",
        "    return {\n",
        "        'critical': critical_points,\n",
        "        'moderate': moderate_points,\n",
        "        'fair': healthy_points,  # Manter compatibilidade\n",
        "        'water': [],\n",
        "        'total_points': total_points,\n",
        "        'generation_method': 'real_ndvi_based',\n",
        "        'generation_params': {\n",
        "            'min_distance': MIN_DISTANCE_POINTS,\n",
        "            'max_points_per_category': max_points_per_category,\n",
        "            'buffer_distance_m': BUFFER_DISTANCE_RIVER,\n",
        "            'buffer_constrained': True,\n",
        "            'real_ndvi_based': True,\n",
        "            'thresholds': {\n",
        "                'critical': NDVI_CRITICAL_THRESHOLD,\n",
        "                'moderate': NDVI_MODERATE_THRESHOLD\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Gerar pontos por categoria (mantendo chaves originais para compatibilidade)\n",
        "    critical_points = generate_points_for_category(\n",
        "        min(20, max_points_per_category), 0.00, 0.19, \"critical\", \"#DC143C\", \"Vegeta√ß√£o muito rala / solo exposto\"\n",
        "    )\n",
        "\n",
        "    moderate_points = generate_points_for_category(\n",
        "        min(50, max_points_per_category), 0.20, 0.49, \"moderate\", \"#FF8C00\", \"Vegeta√ß√£o esparsa / em regenera√ß√£o\"\n",
        "    )\n",
        "\n",
        "    fair_points = generate_points_for_category(\n",
        "        min(30, max_points_per_category), 0.50, 0.79, \"fair\", \"#228B22\", \"Vegeta√ß√£o densa e saud√°vel\"\n",
        "    )\n",
        "\n",
        "    # Calcular estat√≠sticas\n",
        "    total_points = len(critical_points) + len(moderate_points) + len(fair_points)\n",
        "\n",
        "    if total_points == 0:\n",
        "        print(\"‚ùå Nenhum ponto gerado dentro do buffer\")\n",
        "        return None\n",
        "\n",
        "    # Calcular dist√¢ncias m√©dias\n",
        "    all_points = critical_points + moderate_points + fair_points\n",
        "    distances = [p.get('distance_to_river_m', 0) for p in all_points]\n",
        "    avg_distance = np.mean(distances) if distances else 0\n",
        "    max_distance = np.max(distances) if distances else 0\n",
        "\n",
        "    print(f\"\\nüìä Pontos gerados com sucesso:\")\n",
        "    print(f\"   üî¥ Cr√≠ticos: {len(critical_points)}\")\n",
        "    print(f\"   üü° Moderados: {len(moderate_points)}\")\n",
        "    print(f\"   üü® Regulares: {len(fair_points)}\")\n",
        "    print(f\"   üìä Total: {total_points}\")\n",
        "    print(f\"   üìè Dist√¢ncia m√©dia ao rio: {avg_distance:.1f}m\")\n",
        "    print(f\"   üìè Dist√¢ncia m√°xima ao rio: {max_distance:.1f}m\")\n",
        "\n",
        "    return {\n",
        "        'critical': critical_points,\n",
        "        'moderate': moderate_points,\n",
        "        'fair': fair_points,\n",
        "        'water': [],  # N√£o geramos pontos de √°gua nesta vers√£o\n",
        "        'total_points': total_points,\n",
        "        'generation_params': {\n",
        "            'min_distance': MIN_DISTANCE_POINTS,\n",
        "            'max_points_per_category': max_points_per_category,\n",
        "            'buffer_distance_m': BUFFER_DISTANCE_RIVER,\n",
        "            'buffer_constrained': True,\n",
        "            'river_proximity_weighting': True,\n",
        "            'thresholds': {\n",
        "                'critical': NDVI_CRITICAL_THRESHOLD,\n",
        "                'moderate': NDVI_MODERATE_THRESHOLD\n",
        "            },\n",
        "            'spatial_stats': {\n",
        "                'avg_distance_to_river_m': avg_distance,\n",
        "                'max_distance_to_river_m': max_distance\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Executar gera√ß√£o de pontos cr√≠ticos com a vers√£o corrigida\n",
        "if degradation_analysis:\n",
        "    print(\"üöÄ Iniciando gera√ß√£o de pontos cr√≠ticos (VERS√ÉO CORRIGIDA)...\")\n",
        "\n",
        "    critical_points_data = generate_points_from_real_ndvi(\n",
        "        degradation_analysis,\n",
        "        river_buffer_geom,  # Precisa ser definido antes\n",
        "        max_points_per_category=MAX_POINTS_PER_SEVERITY\n",
        "    )\n",
        "\n",
        "    if critical_points_data and critical_points_data['total_points'] > 0:\n",
        "        print(\"‚úÖ Gera√ß√£o de pontos cr√≠ticos conclu√≠da com sucesso\")\n",
        "\n",
        "        # Estat√≠sticas finais\n",
        "        if degradation_analysis and 'statistics' in degradation_analysis:\n",
        "            stats = degradation_analysis['statistics']\n",
        "            gen_params = critical_points_data['generation_params']\n",
        "\n",
        "            print(f\"\\nüìä Resumo da An√°lise (CORRIGIDA):\")\n",
        "            print(f\"   üåä M√©todo: Buffer restrito ao rio ({BUFFER_DISTANCE_RIVER}m)\")\n",
        "            print(f\"   üìà NDVI m√©dio: {stats.get('ndvi_mean', 0):.3f}\")\n",
        "            print(f\"   üìç Pontos gerados: {critical_points_data['total_points']}\")\n",
        "            print(f\"   üìè Dist√¢ncia m√©dia ao rio: {gen_params['spatial_stats']['avg_distance_to_river_m']:.1f}m\")\n",
        "            print(f\"   ‚úÖ Todos os pontos dentro do buffer do rio\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Nenhum ponto cr√≠tico gerado\")\n",
        "        critical_points_data = None\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå An√°lise de degrada√ß√£o n√£o dispon√≠vel para gerar pontos\")\n",
        "    critical_points_data = None\n",
        "\n",
        "print(\"\\nüí° CORRE√á√ÉO APLICADA:\")\n",
        "print(\"   ‚úÖ Pontos gerados APENAS dentro do buffer de 200m do rio\")\n",
        "print(\"   ‚úÖ Coordenadas corretas (n√£o mais no oceano)\")\n",
        "print(\"   ‚úÖ Dist√¢ncias ao rio calculadas e validadas\")\n",
        "print(\"   ‚úÖ Prioriza√ß√£o de pontos cr√≠ticos pr√≥ximos √†s margens\")\n",
        "\n",
        "# @title\n",
        "# ==========================================\n",
        "# ETAPA 6: Exporta√ß√£o de Resultados (GeoJSON e GeoTIFF)\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nüíæ ETAPA 6: Exporta√ß√£o de Resultados\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Configura√ß√µes de exporta√ß√£o - CORRIGIDAS PARA COLAB\n",
        "OUTPUT_DIR = \".\"  # Pasta raiz do Colab (diret√≥rio atual)\n",
        "GEOJSON_FILENAME = \"critical_points_mata_ciliar.geojson\"\n",
        "GEOTIFF_FILENAME = \"ndvi_mata_ciliar_wgs84_normalized.geotiff\"\n",
        "LOG_FILENAME = \"processamento_notebook.log\"\n",
        "\n",
        "def ensure_output_directory(output_dir):\n",
        "    \"\"\"Garante que o diret√≥rio de sa√≠da existe\"\"\"\n",
        "    try:\n",
        "        # Se for diret√≥rio atual (.), n√£o precisa criar\n",
        "        if output_dir == \".\" or output_dir == \"\":\n",
        "            print(f\"üìÅ Salvando na pasta raiz do Colab: {os.getcwd()}\")\n",
        "            return True\n",
        "\n",
        "        # Criar diret√≥rio se n√£o existir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"üìÅ Diret√≥rio criado/verificado: {output_dir}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao criar diret√≥rio {output_dir}: {e}\")\n",
        "        print(f\"üí° Tentando usar diret√≥rio atual como fallback...\")\n",
        "        return True  # Usar diret√≥rio atual como fallback\n",
        "\n",
        "def export_geojson_results(critical_points_data, degradation_analysis, output_path):\n",
        "    \"\"\"Exporta pontos cr√≠ticos para GeoJSON compat√≠vel com a aplica√ß√£o web\"\"\"\n",
        "\n",
        "    if not critical_points_data:\n",
        "        print(\"‚ùå Nenhum dado de pontos cr√≠ticos para exportar\")\n",
        "        return False\n",
        "\n",
        "    print(f\"üìÑ Exportando GeoJSON: {output_path}\")\n",
        "\n",
        "    try:\n",
        "        # Combinar todos os pontos\n",
        "        all_points = []\n",
        "\n",
        "        # Adicionar pontos cr√≠ticos (prioridade m√°xima)\n",
        "        for point in critical_points_data.get('critical', []):\n",
        "            all_points.append({\n",
        "                \"type\": \"Feature\",\n",
        "                \"geometry\": {\n",
        "                    \"type\": \"Point\",\n",
        "                    \"coordinates\": [point['lon'], point['lat']]\n",
        "                },\n",
        "                \"properties\": {\n",
        "                    \"severity\": point['severity'],\n",
        "                    \"ndvi\": point['ndvi'],\n",
        "                    \"description\": point['description'],\n",
        "                    \"type\": \"critical_point\",\n",
        "                    \"level\": point['level'],\n",
        "                    \"color\": point['color'],\n",
        "                    \"label\": point['label'],\n",
        "                    \"distance_to_river_m\": point.get('distance_to_river_m', 0)\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Adicionar pontos moderados\n",
        "        for point in critical_points_data.get('moderate', []):\n",
        "            all_points.append({\n",
        "                \"type\": \"Feature\",\n",
        "                \"geometry\": {\n",
        "                    \"type\": \"Point\",\n",
        "                    \"coordinates\": [point['lon'], point['lat']]\n",
        "                },\n",
        "                \"properties\": {\n",
        "                    \"severity\": point['severity'],\n",
        "                    \"ndvi\": point['ndvi'],\n",
        "                    \"description\": point['description'],\n",
        "                    \"type\": \"moderate_point\",\n",
        "                    \"level\": point['level'],\n",
        "                    \"color\": point['color'],\n",
        "                    \"label\": point['label']\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Adicionar alguns pontos regulares para contexto (limitado)\n",
        "        fair_points = critical_points_data.get('fair', [])[:50]  # M√°ximo 50 pontos regulares\n",
        "        for point in fair_points:\n",
        "            all_points.append({\n",
        "                \"type\": \"Feature\",\n",
        "                \"geometry\": {\n",
        "                    \"type\": \"Point\",\n",
        "                    \"coordinates\": [point['lon'], point['lat']]\n",
        "                },\n",
        "                \"properties\": {\n",
        "                    \"severity\": point['severity'],\n",
        "                    \"ndvi\": point['ndvi'],\n",
        "                    \"description\": point['description'],\n",
        "                    \"type\": \"fair_point\",\n",
        "                    \"level\": point['level'],\n",
        "                    \"color\": point['color'],\n",
        "                    \"label\": point['label']\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Metadados da an√°lise\n",
        "        stats = degradation_analysis['statistics'] if degradation_analysis else {}\n",
        "        generation_params = critical_points_data.get('generation_params', {})\n",
        "\n",
        "        # Criar GeoJSON final\n",
        "        geojson_data = {\n",
        "            \"type\": \"FeatureCollection\",\n",
        "            \"features\": all_points,\n",
        "            \"metadata\": {\n",
        "                \"analysis_date\": datetime.now().isoformat(),\n",
        "                \"data_source\": \"HLS (Harmonized Landsat Sentinel)\",\n",
        "                \"buffer_distance\": f\"{BUFFER_DISTANCE} meters\",\n",
        "                \"thresholds\": generation_params.get('thresholds', {\n",
        "                    \"critical\": NDVI_CRITICAL_THRESHOLD,\n",
        "                    \"moderate\": NDVI_MODERATE_THRESHOLD\n",
        "                }),\n",
        "                \"statistics\": {\n",
        "                    \"total_pixels\": stats.get('total_pixels', 0),\n",
        "                    \"ndvi_min\": stats.get('ndvi_min', 0),\n",
        "                    \"ndvi_max\": stats.get('ndvi_max', 0),\n",
        "                    \"ndvi_mean\": stats.get('ndvi_mean', 0),\n",
        "                    \"critical_fraction\": stats.get('critical_fraction', 0),\n",
        "                    \"moderate_fraction\": stats.get('moderate_fraction', 0),\n",
        "                    \"overall_status\": stats.get('overall_status', 'unknown')\n",
        "                },\n",
        "                \"total_critical_points\": len(critical_points_data.get('critical', [])),\n",
        "                \"total_moderate_points\": len(critical_points_data.get('moderate', [])),\n",
        "                \"total_fair_points\": len(fair_points),\n",
        "                \"processing_params\": {\n",
        "                    \"min_distance_points\": generation_params.get('min_distance', MIN_DISTANCE_POINTS),\n",
        "                    \"sampling_step\": generation_params.get('sampling_step', SAMPLING_STEP),\n",
        "                    \"start_date\": START_DATE,\n",
        "                    \"end_date\": END_DATE,\n",
        "                    \"cloud_coverage_max\": CLOUD_COVERAGE_MAX\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Salvar arquivo\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(geojson_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"‚úÖ GeoJSON exportado com sucesso\")\n",
        "        print(f\"   üìä Total de features: {len(all_points)}\")\n",
        "        print(f\"   üî¥ Pontos cr√≠ticos: {len(critical_points_data.get('critical', []))}\")\n",
        "        print(f\"   üü° Pontos moderados: {len(critical_points_data.get('moderate', []))}\")\n",
        "        print(f\"   üü® Pontos regulares: {len(fair_points)}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao exportar GeoJSON: {e}\")\n",
        "        return False\n",
        "\n",
        "def export_geotiff_results(final_ndvi_data, degradation_analysis, output_path):\n",
        "    \"\"\"Exporta raster NDVI para GeoTIFF\"\"\"\n",
        "\n",
        "    if not final_ndvi_data or 'ndvi' not in final_ndvi_data:\n",
        "        print(\"‚ùå Dados NDVI n√£o dispon√≠veis para exportar\")\n",
        "        return False\n",
        "\n",
        "    if not degradation_analysis:\n",
        "        print(\"‚ùå An√°lise de degrada√ß√£o n√£o dispon√≠vel\")\n",
        "        return False\n",
        "\n",
        "    print(f\"üó∫Ô∏è Exportando GeoTIFF: {output_path}\")\n",
        "\n",
        "    try:\n",
        "        # Usar NDVI recortado da an√°lise de degrada√ß√£o\n",
        "        ndvi_clipped = degradation_analysis['ndvi_clipped']\n",
        "\n",
        "        # Converter para WGS84 se necess√°rio\n",
        "        if ndvi_clipped.rio.crs != 'EPSG:4326':\n",
        "            print(\"   üîÑ Reprojetando para WGS84...\")\n",
        "            ndvi_wgs84 = ndvi_clipped.rio.reproject('EPSG:4326')\n",
        "        else:\n",
        "            ndvi_wgs84 = ndvi_clipped\n",
        "\n",
        "        # Manter valores reais de NDVI (sem normaliza√ß√£o)\n",
        "        ndvi_values = ndvi_wgs84.values\n",
        "        valid_mask = ~np.isnan(ndvi_values)\n",
        "\n",
        "        if np.sum(valid_mask) > 0:\n",
        "            ndvi_min = np.nanmin(ndvi_values)\n",
        "            ndvi_max = np.nanmax(ndvi_values)\n",
        "            print(f\"   üìä Valores reais de NDVI mantidos:\")\n",
        "            print(f\"      - Range: {ndvi_min:.3f} a {ndvi_max:.3f}\")\n",
        "            print(f\"      - Valores originais preservados para an√°lise\")\n",
        "\n",
        "        # Salvar GeoTIFF\n",
        "        ndvi_wgs84.rio.to_raster(\n",
        "            output_path,\n",
        "            driver='GTiff',  # Especificar driver explicitamente\n",
        "            compress='lzw',  # Compress√£o para reduzir tamanho\n",
        "            nodata=np.nan,\n",
        "            dtype='float32'\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ GeoTIFF exportado com sucesso\")\n",
        "        print(f\"   üìê Dimens√µes: {ndvi_wgs84.shape}\")\n",
        "        print(f\"   üó∫Ô∏è CRS: {ndvi_wgs84.rio.crs}\")\n",
        "        print(f\"   üìè Resolu√ß√£o: ~{abs(ndvi_wgs84.rio.resolution()[0]):.0f}m\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao exportar GeoTIFF: {e}\")\n",
        "        return False\n",
        "\n",
        "def create_processing_log(critical_points_data, degradation_analysis, final_ndvi_data, log_path):\n",
        "    \"\"\"Cria log detalhado do processamento\"\"\"\n",
        "\n",
        "    print(f\"üìù Criando log de processamento: {log_path}\")\n",
        "\n",
        "    try:\n",
        "        log_content = []\n",
        "        log_content.append(\"=\" * 80)\n",
        "        log_content.append(f\"HLS.ipynb - Log de Processamento\")\n",
        "        log_content.append(f\"Data/Hora: {datetime.now().isoformat()}\")\n",
        "        log_content.append(\"=\" * 80)\n",
        "\n",
        "        # Par√¢metros de configura√ß√£o\n",
        "        log_content.append(\"\\nüìã CONFIGURA√á√ïES:\")\n",
        "        log_content.append(f\"  - Per√≠odo de an√°lise: {START_DATE} a {END_DATE}\")\n",
        "        log_content.append(f\"  - Cobertura m√°xima de nuvens: {CLOUD_COVERAGE_MAX}%\")\n",
        "        log_content.append(f\"  - Buffer mata ciliar: {BUFFER_DISTANCE}m\")\n",
        "        log_content.append(f\"  - Threshold cr√≠tico: {NDVI_CRITICAL_THRESHOLD}\")\n",
        "        log_content.append(f\"  - Threshold moderado: {NDVI_MODERATE_THRESHOLD}\")\n",
        "        log_content.append(f\"  - Dist√¢ncia m√≠nima entre pontos: {MIN_DISTANCE_POINTS}m\")\n",
        "\n",
        "        # Dados HLS processados\n",
        "        if final_ndvi_data and 'source_items' in final_ndvi_data:\n",
        "            log_content.append(\"\\nüõ∞Ô∏è DADOS HLS PROCESSADOS:\")\n",
        "            for i, item_data in enumerate(final_ndvi_data['source_items']):\n",
        "                item = item_data['item']\n",
        "                stats = item_data['stats']\n",
        "                log_content.append(f\"  {i+1}. {item.collection_id}\")\n",
        "                log_content.append(f\"     Data: {item.properties.get('datetime', 'N/A')[:10]}\")\n",
        "                log_content.append(f\"     Nuvens: {item.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
        "                log_content.append(f\"     NDVI m√©dio: {stats['mean']:.3f}\")\n",
        "                log_content.append(f\"     Pixels v√°lidos: {stats['valid_pixels']:,}\")\n",
        "\n",
        "        # Estat√≠sticas de degrada√ß√£o\n",
        "        if degradation_analysis:\n",
        "            stats = degradation_analysis['statistics']\n",
        "            log_content.append(\"\\nüåä AN√ÅLISE DE DEGRADA√á√ÉO:\")\n",
        "            log_content.append(f\"  - Status geral: {stats['overall_status']}\")\n",
        "            log_content.append(f\"  - NDVI m√©dio: {stats['ndvi_mean']:.3f}\")\n",
        "            log_content.append(f\"  - NDVI m√≠n/m√°x: {stats['ndvi_min']:.3f} / {stats['ndvi_max']:.3f}\")\n",
        "            log_content.append(f\"  - Pixels cr√≠ticos: {stats['critical_pixels']:,} ({stats['critical_fraction']:.1%})\")\n",
        "            log_content.append(f\"  - Pixels moderados: {stats['moderate_pixels']:,} ({stats['moderate_fraction']:.1%})\")\n",
        "            log_content.append(f\"  - Pixels saud√°veis: {stats['healthy_pixels']:,} ({stats['healthy_fraction']:.1%})\")\n",
        "\n",
        "        # Pontos cr√≠ticos gerados\n",
        "        if critical_points_data:\n",
        "            log_content.append(\"\\nüìç PONTOS CR√çTICOS GERADOS:\")\n",
        "            log_content.append(f\"  - Cr√≠ticos: {len(critical_points_data.get('critical', []))}\")\n",
        "            log_content.append(f\"  - Moderados: {len(critical_points_data.get('moderate', []))}\")\n",
        "            log_content.append(f\"  - Regulares: {len(critical_points_data.get('fair', []))}\")\n",
        "            log_content.append(f\"  - √Ågua/Solo: {len(critical_points_data.get('water', []))}\")\n",
        "            log_content.append(f\"  - Total: {critical_points_data['total_points']}\")\n",
        "\n",
        "        # Arquivos de sa√≠da\n",
        "        log_content.append(\"\\nüíæ ARQUIVOS GERADOS:\")\n",
        "        log_content.append(f\"  - GeoJSON: {GEOJSON_FILENAME}\")\n",
        "        log_content.append(f\"  - GeoTIFF: {GEOTIFF_FILENAME}\")\n",
        "        log_content.append(f\"  - Log: {LOG_FILENAME}\")\n",
        "\n",
        "        log_content.append(\"\\n\" + \"=\" * 80)\n",
        "        log_content.append(\"Processamento conclu√≠do com sucesso!\")\n",
        "        log_content.append(\"=\" * 80)\n",
        "\n",
        "        # Salvar log\n",
        "        with open(log_path, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(log_content))\n",
        "\n",
        "        print(\"‚úÖ Log de processamento criado\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao criar log: {e}\")\n",
        "        return False\n",
        "\n",
        "# Executar exporta√ß√£o\n",
        "if critical_points_data and degradation_analysis:\n",
        "    print(\"üöÄ Iniciando exporta√ß√£o de resultados...\")\n",
        "\n",
        "    # Garantir diret√≥rio de sa√≠da\n",
        "    if ensure_output_directory(OUTPUT_DIR):\n",
        "\n",
        "        # Caminhos completos - CORRIGIDOS PARA COLAB\n",
        "        if OUTPUT_DIR == \".\" or OUTPUT_DIR == \"\":\n",
        "            # Salvar diretamente na pasta raiz\n",
        "            geojson_path = GEOJSON_FILENAME\n",
        "            geotiff_path = GEOTIFF_FILENAME\n",
        "            log_path = LOG_FILENAME\n",
        "        else:\n",
        "            # Usar os.path.join se for um diret√≥rio espec√≠fico\n",
        "            geojson_path = os.path.join(OUTPUT_DIR, GEOJSON_FILENAME)\n",
        "            geotiff_path = os.path.join(OUTPUT_DIR, GEOTIFF_FILENAME)\n",
        "            log_path = os.path.join(OUTPUT_DIR, LOG_FILENAME)\n",
        "\n",
        "        print(f\"üìç Caminhos de sa√≠da:\")\n",
        "        print(f\"   üìÑ GeoJSON: {geojson_path}\")\n",
        "        print(f\"   üó∫Ô∏è GeoTIFF: {geotiff_path}\")\n",
        "        print(f\"   üìù Log: {log_path}\")\n",
        "\n",
        "        # Exportar GeoJSON\n",
        "        geojson_success = export_geojson_results(\n",
        "            critical_points_data,\n",
        "            degradation_analysis,\n",
        "            geojson_path\n",
        "        )\n",
        "\n",
        "        # Exportar GeoTIFF\n",
        "        geotiff_success = export_geotiff_results(\n",
        "            final_ndvi_data,\n",
        "            degradation_analysis,\n",
        "            geotiff_path\n",
        "        )\n",
        "\n",
        "        # Criar log\n",
        "        log_success = create_processing_log(\n",
        "            critical_points_data,\n",
        "            degradation_analysis,\n",
        "            final_ndvi_data,\n",
        "            log_path\n",
        "        )\n",
        "\n",
        "        # Resumo final\n",
        "        print(f\"\\nüéâ PROCESSAMENTO HLS CONCLU√çDO!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"‚úÖ GeoJSON: {'Sucesso' if geojson_success else 'Falhou'}\")\n",
        "        print(f\"‚úÖ GeoTIFF: {'Sucesso' if geotiff_success else 'Falhou'}\")\n",
        "        print(f\"‚úÖ Log: {'Sucesso' if log_success else 'Falhou'}\")\n",
        "\n",
        "        # Verificar se arquivos foram realmente criados\n",
        "        print(f\"\\nüîç VERIFICA√á√ÉO DE ARQUIVOS:\")\n",
        "        files_created = []\n",
        "\n",
        "        if os.path.exists(geojson_path):\n",
        "            size_mb = os.path.getsize(geojson_path) / (1024*1024)\n",
        "            print(f\"   ‚úÖ {geojson_path} ({size_mb:.2f} MB)\")\n",
        "            files_created.append(geojson_path)\n",
        "        else:\n",
        "            print(f\"   ‚ùå {geojson_path} N√ÉO ENCONTRADO\")\n",
        "\n",
        "        if os.path.exists(geotiff_path):\n",
        "            size_mb = os.path.getsize(geotiff_path) / (1024*1024)\n",
        "            print(f\"   ‚úÖ {geotiff_path} ({size_mb:.2f} MB)\")\n",
        "            files_created.append(geotiff_path)\n",
        "        else:\n",
        "            print(f\"   ‚ùå {geotiff_path} N√ÉO ENCONTRADO\")\n",
        "\n",
        "        if os.path.exists(log_path):\n",
        "            size_kb = os.path.getsize(log_path) / 1024\n",
        "            print(f\"   ‚úÖ {log_path} ({size_kb:.1f} KB)\")\n",
        "            files_created.append(log_path)\n",
        "        else:\n",
        "            print(f\"   ‚ùå {log_path} N√ÉO ENCONTRADO\")\n",
        "\n",
        "        if files_created:\n",
        "            print(f\"\\nüìÅ Arquivos criados na pasta raiz do Colab:\")\n",
        "            for file_path in files_created:\n",
        "                print(f\"   üìÑ {file_path}\")\n",
        "\n",
        "            print(f\"\\nüåê Pr√≥ximos passos:\")\n",
        "            print(f\"   1. Fazer download dos arquivos do Colab\")\n",
        "            print(f\"   2. Copiar para a pasta public/ do seu projeto\")\n",
        "            print(f\"   3. Testar visualiza√ß√£o no AOIViewer.jsx\")\n",
        "            print(f\"   4. Validar pontos cr√≠ticos no mapa web\")\n",
        "\n",
        "            print(f\"\\nüí° Para fazer download no Colab:\")\n",
        "            print(f\"   from google.colab import files\")\n",
        "            for file_path in files_created:\n",
        "                print(f\"   files.download('{file_path}')\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå NENHUM ARQUIVO FOI CRIADO!\")\n",
        "            print(f\"   Verifique os erros nas etapas anteriores\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Falha ao criar diret√≥rio de sa√≠da\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Dados insuficientes para exporta√ß√£o\")\n",
        "    print(\"   Verifique se todas as etapas anteriores foram executadas com sucesso\")\n",
        "\n",
        "# @title\n",
        "# ==========================================\n",
        "# C√âLULA ADICIONAL: Download dos Arquivos Gerados\n",
        "# ==========================================\n",
        "# Execute esta c√©lula para fazer download dos arquivos criados\n",
        "\n",
        "print(\"üì• DOWNLOAD DOS ARQUIVOS GERADOS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import os\n",
        "\n",
        "    # Lista de arquivos para download\n",
        "    arquivos_download = [\n",
        "        \"critical_points_mata_ciliar.geojson\",\n",
        "        \"ndvi_mata_ciliar_wgs84_normalized.geotiff\",\n",
        "        \"processamento_notebook.log\"\n",
        "    ]\n",
        "\n",
        "    arquivos_encontrados = []\n",
        "\n",
        "    # Verificar quais arquivos existem\n",
        "    print(\"üîç Verificando arquivos dispon√≠veis...\")\n",
        "    for arquivo in arquivos_download:\n",
        "        if os.path.exists(arquivo):\n",
        "            size_mb = os.path.getsize(arquivo) / (1024*1024)\n",
        "            print(f\"   ‚úÖ {arquivo} ({size_mb:.2f} MB)\")\n",
        "            arquivos_encontrados.append(arquivo)\n",
        "        else:\n",
        "            print(f\"   ‚ùå {arquivo} - N√£o encontrado\")\n",
        "\n",
        "    if arquivos_encontrados:\n",
        "        print(f\"\\nüì• Iniciando download de {len(arquivos_encontrados)} arquivo(s)...\")\n",
        "\n",
        "        for arquivo in arquivos_encontrados:\n",
        "            try:\n",
        "                print(f\"   üìÑ Baixando {arquivo}...\")\n",
        "                files.download(arquivo)\n",
        "                print(f\"   ‚úÖ {arquivo} baixado com sucesso!\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Erro ao baixar {arquivo}: {e}\")\n",
        "\n",
        "        print(f\"\\nüéâ Download conclu√≠do!\")\n",
        "        print(f\"üí° Os arquivos foram baixados para sua pasta de Downloads\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ùå Nenhum arquivo encontrado para download\")\n",
        "        print(\"   Execute primeiro todas as c√©lulas anteriores do notebook\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Esta c√©lula s√≥ funciona no Google Colab\")\n",
        "    print(\"üí° Se estiver usando outro ambiente:\")\n",
        "    print(\"   - Os arquivos j√° est√£o salvos na pasta atual\")\n",
        "    print(\"   - Voc√™ pode copi√°-los manualmente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro durante o download: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"üìã INSTRU√á√ïES PARA USO DOS ARQUIVOS:\")\n",
        "print(\"1. üìÑ critical_points_mata_ciliar.geojson\")\n",
        "print(\"   ‚Üí Copiar para: public/ do seu projeto\")\n",
        "print(\"   ‚Üí Usado por: AOIViewer.jsx para mostrar pontos cr√≠ticos\")\n",
        "print(\"\")\n",
        "print(\"2. üó∫Ô∏è ndvi_mata_ciliar_wgs84_normalized.geotiff\")\n",
        "print(\"   ‚Üí Copiar para: public/ do seu projeto\")\n",
        "print(\"   ‚Üí Usado por: Visualiza√ß√£o de raster NDVI\")\n",
        "print(\"\")\n",
        "print(\"3. üìù processamento_notebook.log\")\n",
        "print(\"   ‚Üí Arquivo de log com detalhes do processamento\")\n",
        "print(\"   ‚Üí Para refer√™ncia e debugging\")\n",
        "print(\"=\" * 50)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
